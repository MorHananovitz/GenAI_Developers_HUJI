{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1_kb3erex7Cq"
      },
      "source": [
        "# Introducion to GenAI\n",
        "\n",
        "Mor Hananovitz (c)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install langchain_openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "VmCkkGWkyZ-o"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/mor.h/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "# Load environment variables (you'll need to create a .env file with your OpenAI API key)\n",
        "load_dotenv()\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Prompt Engineering 101"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install langchain_openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "-OmyN8ntBQ6U"
      },
      "outputs": [],
      "source": [
        "gpt4o_chat = ChatOpenAI(model=\"gpt-4o-2024-08-06\", temperature=0, max_tokens=256)\n",
        "gpt35_chat = ChatOpenAI(model=\"gpt-3.5-turbo-0125\", temperature=0, max_tokens=256)\n",
        "gpt_4o_mini_chat = ChatOpenAI(model=\"o4-mini-2025-04-16\", max_tokens=256)\n",
        "gpt_4_1_nano_chat = ChatOpenAI(model=\"gpt-4.1-nano-2025-04-14\", max_tokens=256)\n",
        "gpt_41_chat = ChatOpenAI(model=\"gpt-4.1-2025-04-14\", max_tokens=256)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Available OpenAI Models:\n",
            "==================================================\n",
            "\n",
            "gpt-4o\n",
            "  Type: text_generation\n",
            "  Description: Most capable GPT model, multimodal (text + images)\n",
            "  Context Window: 128,000 tokens\n",
            "  Input Price: $2.5/1M tokens\n",
            "  Output Price: $10.0/1M tokens\n",
            "\n",
            "gpt-4o-mini\n",
            "  Type: text_generation\n",
            "  Description: Faster, cheaper version of GPT-4o\n",
            "  Context Window: 128,000 tokens\n",
            "  Input Price: $0.15/1M tokens\n",
            "  Output Price: $0.6/1M tokens\n",
            "\n",
            "gpt-4.1\n",
            "  Type: text_generation\n",
            "  Description: Latest GPT model with improved capabilities\n",
            "  Context Window: 200,000 tokens\n",
            "  Input Price: $10.0/1M tokens\n",
            "  Output Price: $30.0/1M tokens\n",
            "\n",
            "gpt-4.1-mini\n",
            "  Type: text_generation\n",
            "  Description: Smaller, faster version of GPT-4.1\n",
            "  Context Window: 200,000 tokens\n",
            "  Input Price: $1.0/1M tokens\n",
            "  Output Price: $4.0/1M tokens\n",
            "\n",
            "gpt-4.1-nano\n",
            "  Type: text_generation\n",
            "  Description: Ultra-fast, cost-effective model\n",
            "  Context Window: 200,000 tokens\n",
            "  Input Price: $0.25/1M tokens\n",
            "  Output Price: $1.0/1M tokens\n",
            "\n",
            "gpt-3.5-turbo\n",
            "  Type: text_generation\n",
            "  Description: Legacy model, still capable for many tasks\n",
            "  Context Window: 16,385 tokens\n",
            "  Input Price: $0.5/1M tokens\n",
            "  Output Price: $1.5/1M tokens\n",
            "\n",
            "o3\n",
            "  Type: reasoning\n",
            "  Description: Most advanced reasoning model for complex problems\n",
            "  Context Window: 200,000 tokens\n",
            "  Input Price: $60.0/1M tokens\n",
            "  Output Price: $240.0/1M tokens\n",
            "\n",
            "o4-mini\n",
            "  Type: reasoning\n",
            "  Description: Powerful reasoning model, more affordable than o3\n",
            "  Context Window: 200,000 tokens\n",
            "  Input Price: $1.1/1M tokens\n",
            "  Output Price: $4.4/1M tokens\n",
            "\n",
            "whisper-1\n",
            "  Type: audio_transcription\n",
            "  Description: Speech-to-text transcription and translation\n",
            "  Price: $0.006/minute\n",
            "\n",
            "dall-e-3\n",
            "  Type: image_generation\n",
            "  Description: High-quality image generation\n",
            "\n",
            "gpt-image-1\n",
            "  Type: image_generation\n",
            "  Description: Latest image generation model, successor to DALL-E\n",
            "  Input Price: $5.0/1M tokens\n",
            "  Output Price: $40.0/1M tokens\n",
            "\n",
            "text-embedding-3-large\n",
            "  Type: embeddings\n",
            "  Description: Most capable embedding model\n",
            "\n",
            "text-embedding-3-small\n",
            "  Type: embeddings\n",
            "  Description: Efficient embedding model\n",
            "\n",
            "text-embedding-ada-002\n",
            "  Type: embeddings\n",
            "  Description: Legacy embedding model\n"
          ]
        }
      ],
      "source": [
        "# Available OpenAI Models (as of 2025)\n",
        "available_models = {\n",
        "    # GPT Models - Text Generation\n",
        "    \"gpt-4o\": {\n",
        "        \"type\": \"text_generation\",\n",
        "        \"description\": \"Most capable GPT model, multimodal (text + images)\",\n",
        "        \"context_window\": 128000,\n",
        "        \"input_price_per_1m_tokens\": 2.50,\n",
        "        \"output_price_per_1m_tokens\": 10.00\n",
        "    },\n",
        "    \"gpt-4o-mini\": {\n",
        "        \"type\": \"text_generation\", \n",
        "        \"description\": \"Faster, cheaper version of GPT-4o\",\n",
        "        \"context_window\": 128000,\n",
        "        \"input_price_per_1m_tokens\": 0.15,\n",
        "        \"output_price_per_1m_tokens\": 0.60\n",
        "    },\n",
        "    \"gpt-4.1\": {\n",
        "        \"type\": \"text_generation\",\n",
        "        \"description\": \"Latest GPT model with improved capabilities\",\n",
        "        \"context_window\": 200000,\n",
        "        \"input_price_per_1m_tokens\": 10.00,\n",
        "        \"output_price_per_1m_tokens\": 30.00\n",
        "    },\n",
        "    \"gpt-4.1-mini\": {\n",
        "        \"type\": \"text_generation\",\n",
        "        \"description\": \"Smaller, faster version of GPT-4.1\",\n",
        "        \"context_window\": 200000,\n",
        "        \"input_price_per_1m_tokens\": 1.00,\n",
        "        \"output_price_per_1m_tokens\": 4.00\n",
        "    },\n",
        "    \"gpt-4.1-nano\": {\n",
        "        \"type\": \"text_generation\",\n",
        "        \"description\": \"Ultra-fast, cost-effective model\",\n",
        "        \"context_window\": 200000,\n",
        "        \"input_price_per_1m_tokens\": 0.25,\n",
        "        \"output_price_per_1m_tokens\": 1.00\n",
        "    },\n",
        "    \"gpt-3.5-turbo\": {\n",
        "        \"type\": \"text_generation\",\n",
        "        \"description\": \"Legacy model, still capable for many tasks\",\n",
        "        \"context_window\": 16385,\n",
        "        \"input_price_per_1m_tokens\": 0.50,\n",
        "        \"output_price_per_1m_tokens\": 1.50\n",
        "    },\n",
        "    \n",
        "    # Reasoning Models\n",
        "    \"o3\": {\n",
        "        \"type\": \"reasoning\",\n",
        "        \"description\": \"Most advanced reasoning model for complex problems\",\n",
        "        \"context_window\": 200000,\n",
        "        \"input_price_per_1m_tokens\": 60.00,\n",
        "        \"output_price_per_1m_tokens\": 240.00\n",
        "    },\n",
        "    \"o4-mini\": {\n",
        "        \"type\": \"reasoning\",\n",
        "        \"description\": \"Powerful reasoning model, more affordable than o3\",\n",
        "        \"context_window\": 200000,\n",
        "        \"input_price_per_1m_tokens\": 1.10,\n",
        "        \"output_price_per_1m_tokens\": 4.40\n",
        "    },\n",
        "    \n",
        "    # Audio Models\n",
        "    \"whisper-1\": {\n",
        "        \"type\": \"audio_transcription\",\n",
        "        \"description\": \"Speech-to-text transcription and translation\",\n",
        "        \"price_per_minute\": 0.006\n",
        "    },\n",
        "    \n",
        "    # Image Generation Models\n",
        "    \"dall-e-3\": {\n",
        "        \"type\": \"image_generation\",\n",
        "        \"description\": \"High-quality image generation\",\n",
        "        \"price_1024x1024_standard\": 0.040,\n",
        "        \"price_1024x1024_hd\": 0.080\n",
        "    },\n",
        "    \"gpt-image-1\": {\n",
        "        \"type\": \"image_generation\",\n",
        "        \"description\": \"Latest image generation model, successor to DALL-E\",\n",
        "        \"input_price_per_1m_tokens\": 5.00,\n",
        "        \"output_price_per_1m_tokens\": 40.00\n",
        "    },\n",
        "    \n",
        "    # Embedding Models\n",
        "    \"text-embedding-3-large\": {\n",
        "        \"type\": \"embeddings\",\n",
        "        \"description\": \"Most capable embedding model\",\n",
        "        \"dimensions\": 3072,\n",
        "        \"price_per_1m_tokens\": 0.13\n",
        "    },\n",
        "    \"text-embedding-3-small\": {\n",
        "        \"type\": \"embeddings\", \n",
        "        \"description\": \"Efficient embedding model\",\n",
        "        \"dimensions\": 1536,\n",
        "        \"price_per_1m_tokens\": 0.02\n",
        "    },\n",
        "    \"text-embedding-ada-002\": {\n",
        "        \"type\": \"embeddings\",\n",
        "        \"description\": \"Legacy embedding model\",\n",
        "        \"dimensions\": 1536,\n",
        "        \"price_per_1m_tokens\": 0.10\n",
        "    }\n",
        "}\n",
        "\n",
        "print(\"Available OpenAI Models:\")\n",
        "print(\"=\" * 50)\n",
        "for model_name, details in available_models.items():\n",
        "    print(f\"\\n{model_name}\")\n",
        "    print(f\"  Type: {details['type']}\")\n",
        "    print(f\"  Description: {details['description']}\")\n",
        "    if 'context_window' in details:\n",
        "        print(f\"  Context Window: {details['context_window']:,} tokens\")\n",
        "    if 'input_price_per_1m_tokens' in details:\n",
        "        print(f\"  Input Price: ${details['input_price_per_1m_tokens']}/1M tokens\")\n",
        "    if 'output_price_per_1m_tokens' in details:\n",
        "        print(f\"  Output Price: ${details['output_price_per_1m_tokens']}/1M tokens\")\n",
        "    if 'price_per_minute' in details:\n",
        "        print(f\"  Price: ${details['price_per_minute']}/minute\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P4WFdQo4BkuH"
      },
      "source": [
        "### Prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "21wBbc57Wk5o",
        "outputId": "7df278cc-72d9-4ad6-ac64-64a5eddcef5d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[31mPrompt: hi what is your name?\u001b[0m\n",
            "\n",
            "\u001b[34mResponse: Hello! I don't have a personal name, but you can call me ChatGPT. How can I assist you today?\u001b[0m \n"
          ]
        }
      ],
      "source": [
        "prompt = \"\"\"hi what is your name?\"\"\"\n",
        "\n",
        "#respone_name =llm_name.invoke(promt).comtent \n",
        "\n",
        "#response = gpt35_chat.invoke(prompt).content\n",
        "#response = gpt_4o_mini_chat.invoke(prompt).content\n",
        "response = gpt_4_1_nano_chat.invoke(prompt).content\n",
        "\n",
        "\n",
        "\n",
        "print(f\"\\033[31mPrompt: {prompt}\\033[0m\")\n",
        "print(\"\")\n",
        "print(f\"\\033[34mResponse: {response}\\033[0m \")\n",
        "\n",
        "#Response 3.5: Hello! I am a language model AI assistant and I do not have a personal name. You can just call me Assistant. How can I assist you today? \n",
        "#Response 4o mini: Hello! I’m ChatGPT, an AI language model created by OpenAI. How can I help you today? \n",
        "#Response 4.1 nano: Hello! I don't have a personal name, but you can call me ChatGPT. How can I assist you today? \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Shots Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GvjKy03Ff1og"
      },
      "source": [
        "Zero-Shot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0v89XvUQ0hVo",
        "outputId": "ac7a88b1-5bde-475e-9937-d2a0a4a0ed38"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[31mPrompt: Classify the following food items into three groups: A, B, and C.\n",
            "\n",
            "Items: Carrot, Chicken, Spinach, Beef, Banana, Lettuce, Pork, Milk, Butter\u001b[0m\n",
            "\n",
            "\u001b[34mResponse: Group A: Carrot, Spinach, Banana, Lettuce, Milk\n",
            "\n",
            "Group B: Chicken, Beef, Pork\n",
            "\n",
            "Group C: Butter\u001b[0m \n"
          ]
        }
      ],
      "source": [
        "prompt = \"\"\"Classify the following food items into three groups: A, B, and C.\n",
        "\n",
        "Items: Carrot, Chicken, Spinach, Beef, Banana, Lettuce, Pork, Milk, Butter\"\"\"\n",
        "\n",
        "response = gpt35_chat.invoke(prompt).content\n",
        "\n",
        "#response = gpt4o_chat.invoke(prompt).content\n",
        "\n",
        "print(f\"\\033[31mPrompt: {prompt}\\033[0m\")\n",
        "print(\"\")\n",
        "print(f\"\\033[34mResponse: {response}\\033[0m \")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[31mPrompt: Classify the following food items into three groups: A, B, and C.\n",
            "\n",
            "Items: Carrot, Chicken, Spinach, Beef, Banana, Lettuce, Pork, Milk, Butter\u001b[0m\n",
            "\n",
            "\u001b[34mResponse: Here is one way to classify the food items into three groups:\n",
            "\n",
            "Group A (Vegetables):\n",
            "- Carrot\n",
            "- Spinach\n",
            "- Lettuce\n",
            "\n",
            "Group B (Meats):\n",
            "- Chicken\n",
            "- Beef\n",
            "- Pork\n",
            "\n",
            "Group C (Dairy and Fruits):\n",
            "- Banana\n",
            "- Milk\n",
            "- Butter\u001b[0m \n"
          ]
        }
      ],
      "source": [
        "prompt = \"\"\"Classify the following food items into three groups: A, B, and C.\n",
        "\n",
        "Items: Carrot, Chicken, Spinach, Beef, Banana, Lettuce, Pork, Milk, Butter\"\"\"\n",
        "\n",
        "response = gpt4o_chat.invoke(prompt).content\n",
        "\n",
        "\n",
        "print(f\"\\033[31mPrompt: {prompt}\\033[0m\")\n",
        "print(\"\")\n",
        "print(f\"\\033[34mResponse: {response}\\033[0m \")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pwaybKUUgGff"
      },
      "source": [
        "One-Shot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rZitniNgs12_",
        "outputId": "ea973e06-9731-430c-8665-09ca94e0529c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[31mPrompt: Classify the following food items into three groups: A, B, and C.\n",
            "\n",
            "Example:\n",
            "Apple → A\n",
            "\n",
            "Items: Carrot, Chicken, Spinach, Beef, Banana, Lettuce, Pork, Milk, Butter\u001b[0m\n",
            "\n",
            "\u001b[34mResponse: Carrot, Spinach, Lettuce → A\n",
            "Chicken, Beef, Pork → B\n",
            "Banana, Milk, Butter → C\u001b[0m \n"
          ]
        }
      ],
      "source": [
        "prompt = \"\"\"Classify the following food items into three groups: A, B, and C.\n",
        "\n",
        "Example:\n",
        "Apple → A\n",
        "\n",
        "Items: Carrot, Chicken, Spinach, Beef, Banana, Lettuce, Pork, Milk, Butter\"\"\"\n",
        "\n",
        "\n",
        "response = gpt35_chat.invoke(prompt).content\n",
        "\n",
        "#response = gpt4o_chat.invoke(prompt).content\n",
        "\n",
        "print(f\"\\033[31mPrompt: {prompt}\\033[0m\")\n",
        "print(\"\")\n",
        "print(f\"\\033[34mResponse: {response}\\033[0m \")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[31mPrompt: Classify the following food items into three groups: A, B, and C.\n",
            "\n",
            "Example:\n",
            "Apple → A\n",
            "\n",
            "Items: Carrot, Chicken, Spinach, Beef, Banana, Lettuce, Pork, Milk, Butter\u001b[0m\n",
            "\n",
            "\u001b[34mResponse: To classify the food items into three groups, we can use the following criteria:\n",
            "\n",
            "- Group A: Fruits and vegetables\n",
            "- Group B: Meats\n",
            "- Group C: Dairy products\n",
            "\n",
            "Using these criteria, the classification would be:\n",
            "\n",
            "- Carrot → A\n",
            "- Chicken → B\n",
            "- Spinach → A\n",
            "- Beef → B\n",
            "- Banana → A\n",
            "- Lettuce → A\n",
            "- Pork → B\n",
            "- Milk → C\n",
            "- Butter → C\u001b[0m \n"
          ]
        }
      ],
      "source": [
        "prompt = \"\"\"Classify the following food items into three groups: A, B, and C.\n",
        "\n",
        "Example:\n",
        "Apple → A\n",
        "\n",
        "Items: Carrot, Chicken, Spinach, Beef, Banana, Lettuce, Pork, Milk, Butter\"\"\"\n",
        "\n",
        "\n",
        "response = gpt4o_chat.invoke(prompt).content\n",
        "\n",
        "print(f\"\\033[31mPrompt: {prompt}\\033[0m\")\n",
        "print(\"\")\n",
        "print(f\"\\033[34mResponse: {response}\\033[0m \")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5f7iDJe3gPfu"
      },
      "source": [
        "Few-Shot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IUxQZ1pkgPl0",
        "outputId": "16463899-e7af-475d-ff93-7693e1c06d76"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[31mPrompt: Classify the following food items into three groups: A, B, and C.\n",
            "Items: Spinach, Lettuce, Pork, Cheese, Yogurt, Pinnaple, Whip cream, Peach, eggs\n",
            "\n",
            "Examples:\n",
            "Apple → A\n",
            "Carrot → A\n",
            "Chicken → B\n",
            "Banana → A\n",
            "Milk - C\n",
            "Butter - C\n",
            "\n",
            "\n",
            "\u001b[0m\n",
            "\n",
            "\u001b[34mResponse: Group A: Spinach, Lettuce, Pineapple, Peach\n",
            "Group B: Pork, Eggs\n",
            "Group C: Cheese, Yogurt, Whip cream\u001b[0m \n"
          ]
        }
      ],
      "source": [
        "prompt = \"\"\"Classify the following food items into three groups: A, B, and C.\n",
        "Items: Spinach, Lettuce, Pork, Cheese, Yogurt, Pinnaple, Whip cream, Peach, eggs\n",
        "\n",
        "Examples:\n",
        "Apple → A\n",
        "Carrot → A\n",
        "Chicken → B\n",
        "Banana → A\n",
        "Milk - C\n",
        "Butter - C\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "response = gpt35_chat.invoke(prompt).content\n",
        "\n",
        "#response = gpt4o_chat.invoke(prompt).content\n",
        "\n",
        "\n",
        "print(f\"\\033[31mPrompt: {prompt}\\033[0m\")\n",
        "print(\"\")\n",
        "print(f\"\\033[34mResponse: {response}\\033[0m \")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[31mPrompt: Classify the following food items into three groups: A, B, and C.\n",
            "Items: Spinach, Lettuce, Pork, Cheese, Yogurt, Pinnaple, Whip cream, Peach, eggs\n",
            "\n",
            "Examples:\n",
            "Apple → A\n",
            "Carrot → A\n",
            "Chicken → B\n",
            "Banana → A\n",
            "Milk - C\n",
            "Butter - C\n",
            "\n",
            "Can you also label the groups?\n",
            "\u001b[0m\n",
            "\n",
            "\u001b[34mResponse: Certainly! Based on the examples provided, here is the classification of the food items into groups A, B, and C:\n",
            "\n",
            "- **Group A (Fruits and Vegetables):**\n",
            "  - Spinach\n",
            "  - Lettuce\n",
            "  - Pineapple\n",
            "  - Peach\n",
            "\n",
            "- **Group B (Meat and Protein):**\n",
            "  - Pork\n",
            "  - Eggs\n",
            "\n",
            "- **Group C (Dairy and Dairy Products):**\n",
            "  - Cheese\n",
            "  - Yogurt\n",
            "  - Whip cream\n",
            "\n",
            "These classifications are based on the nature of the items and their similarity to the examples given.\u001b[0m \n"
          ]
        }
      ],
      "source": [
        "prompt = \"\"\"Classify the following food items into three groups: A, B, and C.\n",
        "Items: Spinach, Lettuce, Pork, Cheese, Yogurt, Pinnaple, Whip cream, Peach, eggs\n",
        "\n",
        "Examples:\n",
        "Apple → A\n",
        "Carrot → A\n",
        "Chicken → B\n",
        "Banana → A\n",
        "Milk - C\n",
        "Butter - C\n",
        "\n",
        "Can you also label the groups?\n",
        "\"\"\"\n",
        "\n",
        "response = gpt4o_chat.invoke(prompt).content\n",
        "\n",
        "\n",
        "print(f\"\\033[31mPrompt: {prompt}\\033[0m\")\n",
        "print(\"\")\n",
        "print(f\"\\033[34mResponse: {response}\\033[0m \")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LLHAAClSicNa"
      },
      "source": [
        "### Chain of Thought Prompting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VORxDF2Vicde",
        "outputId": "5d4820b9-579c-4ba8-80c7-2f3ac73d3c6e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[31mPrompt: If Sarah had 8 pencils, gave 3 to her friend, then bought 6 more pencils, but 2 of the new pencils were broken, how many pencils does Sarah have now?\n",
            "\n",
            "\u001b[0m\n",
            "\n",
            "\u001b[34mResponse: Sarah initially had 8 pencils. She gave 3 to her friend, leaving her with 8 - 3 = 5 pencils. She then bought 6 more pencils, bringing her total to 5 + 6 = 11 pencils. However, 2 of the new pencils were broken, so she effectively has 11 - 2 = 9 usable pencils. Therefore, Sarah has 9 pencils now.\u001b[0m \n"
          ]
        }
      ],
      "source": [
        "prompt = \"\"\"If Sarah had 8 pencils, gave 3 to her friend, then bought 6 more pencils, but 2 of the new pencils were broken, how many pencils does Sarah have now?\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "#response = gpt35_chat.invoke(prompt).content\n",
        "\n",
        "response = gpt4o_chat.invoke(prompt).content\n",
        "\n",
        "\n",
        "print(f\"\\033[31mPrompt: {prompt}\\033[0m\")\n",
        "print(\"\")\n",
        "print(f\"\\033[34mResponse: {response}\\033[0m \")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vdj8xUZFjzOu",
        "outputId": "581e0fe3-b444-40d8-eb42-c97e55938f8a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[31mPrompt: If Sarah had 8 pencils, gave 3 to her friend, then bought 6 more pencils, but 2 of the new pencils were broken, how many pencils does Sarah have now?\n",
            "\n",
            "Let's break this down step by step.\n",
            "\n",
            "\n",
            "\n",
            "\u001b[0m\n",
            "\n",
            "\u001b[34mResponse: 1. Sarah starts with 8 pencils.\n",
            "2. She gives 3 pencils to her friend, so she now has 8 - 3 = 5 pencils.\n",
            "3. She buys 6 more pencils, so she now has 5 + 6 = 11 pencils.\n",
            "4. However, 2 of the new pencils were broken, so she now has 11 - 2 = 9 pencils.\n",
            "\n",
            "Therefore, Sarah now has 9 pencils.\u001b[0m \n"
          ]
        }
      ],
      "source": [
        "prompt = \"\"\"If Sarah had 8 pencils, gave 3 to her friend, then bought 6 more pencils, but 2 of the new pencils were broken, how many pencils does Sarah have now?\n",
        "\n",
        "Let's break this down step by step.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "response = gpt35_chat.invoke(prompt).content\n",
        "\n",
        "#response = gpt4o_chat.invoke(prompt).content\n",
        "\n",
        "\n",
        "print(f\"\\033[31mPrompt: {prompt}\\033[0m\")\n",
        "print(\"\")\n",
        "print(f\"\\033[34mResponse: {response}\\033[0m \")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Prompt Format Example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "llm = ChatOpenAI(model=\"gpt-4.1-nano-2025-04-14\", max_tokens=256)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "        Name  Age  Score\n",
            "0   Person_1   56  39.52\n",
            "1   Person_2   69  92.67\n",
            "2   Person_3   46  72.73\n",
            "3   Person_4   32  32.65\n",
            "4   Person_5   60  57.04\n",
            "5   Person_6   25  52.08\n",
            "6   Person_7   78  96.12\n",
            "7   Person_8   38  84.45\n",
            "8   Person_9   56  74.73\n",
            "9  Person_10   75  53.97\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Create a random dataset with 3 columns\n",
        "np.random.seed(42)  # For reproducible results\n",
        "\n",
        "data = {\n",
        "    'Name': [f'Person_{i}' for i in range(1, 101)],\n",
        "    'Age': np.random.randint(18, 80, 100),\n",
        "    'Score': np.random.uniform(0, 100, 100).round(2)\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "print(df.head(10))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 100 entries, 0 to 99\n",
            "Data columns (total 3 columns):\n",
            " #   Column  Non-Null Count  Dtype  \n",
            "---  ------  --------------  -----  \n",
            " 0   Name    100 non-null    object \n",
            " 1   Age     100 non-null    int64  \n",
            " 2   Score   100 non-null    float64\n",
            "dtypes: float64(1), int64(1), object(1)\n",
            "memory usage: 2.5+ KB\n"
          ]
        }
      ],
      "source": [
        "info = df.info()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [],
      "source": [
        "stats = df.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [],
      "source": [
        "role = \"\"\"you are a data analyst\"\"\"\n",
        "\n",
        "style  = \"\"\"short and concise\"\"\"\n",
        "\n",
        "format = \"\"\"table for the results and short explenation for the data\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[31mPrompt: \n",
            "you are a you are a data analyst. Your task it to analize the dataframe:           Name  Age  Score\n",
            "0     Person_1   56  39.52\n",
            "1     Person_2   69  92.67\n",
            "2     Person_3   46  72.73\n",
            "3     Person_4   32  32.65\n",
            "4     Person_5   60  57.04\n",
            "..         ...  ...    ...\n",
            "95   Person_96   46  41.10\n",
            "96   Person_97   35   3.31\n",
            "97   Person_98   43  34.51\n",
            "98   Person_99   61  63.44\n",
            "99  Person_100   51  68.07\n",
            "\n",
            "[100 rows x 3 columns] and to average all numeric columns - for context use the following information: None,               Age      Score\n",
            "count  100.000000  100.00000\n",
            "mean    50.270000   48.57180\n",
            "std     19.176403   27.56722\n",
            "min     19.000000    0.05000\n",
            "25%     34.750000   26.41250\n",
            "50%     51.500000   46.59500\n",
            "75%     68.000000   69.49750\n",
            "max     79.000000   99.77000.\n",
            "\n",
            "output style: short and concise\n",
            "output format: table for the results and short explenation for the data\n",
            "\u001b[0m\n",
            "\n",
            "\u001b[34mResponse: | Column | Average Value |\n",
            "|---------|--------------|\n",
            "| Age     | 50.27        |\n",
            "| Score   | 48.57        |\n",
            "\n",
            "The data consists of 100 individuals with ages ranging from 19 to 79, averaging around 50 years. Scores vary widely, averaging approximately 48.57, indicating diverse performance levels across the dataset.\u001b[0m \n"
          ]
        }
      ],
      "source": [
        "prompt = f\"\"\"\n",
        "you are a {role}. Your task it to analize the dataframe: {df} and to average all numeric columns - for context use the following information: {info}, {stats}.\n",
        "\n",
        "output style: {style}\n",
        "output format: {format}\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "response = llm.invoke(prompt).content\n",
        "\n",
        "\n",
        "print(f\"\\033[31mPrompt: {prompt}\\033[0m\")\n",
        "print(\"\")\n",
        "print(f\"\\033[34mResponse: {response}\\033[0m \")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[31mPrompt: You are a 17th century rationalist philosopher. \n",
            "    Your task is to answer the following question in the specified style and format.\n",
            "\n",
            "\n",
            "    Style: You have a very deep and complex knowledge of philosophy and you are able to explain it in a way that is easy to understand.\n",
            "\n",
            "    Format: You answer should be a rap song.\n",
            "\n",
            "\n",
            "    Question: what is the meaning of life?\u001b[0m\n",
            "\n",
            "\u001b[34mResponse: Yo, listen up, I’ma break it down deep,  \n",
            "The meaning of life, it ain’t just to sleep,  \n",
            "It’s that spark in the soul, that rational fire,  \n",
            "To seek truth and knowledge, to lift us higher.  \n",
            "  \n",
            "From Descartes’ mind, I think, therefore I am,  \n",
            "Our essence’s reason, that’s who I am,  \n",
            "We question, explore, through doubt we refine,  \n",
            "Finding the divine in the reasoned design.  \n",
            "  \n",
            "Life’s about the quest, the pursuit of the good,  \n",
            "Living with virtue, doing what we should,  \n",
            "Not just fleeting pleasures or superficial gains,  \n",
            "But eternal truths that lessen our pains.  \n",
            "  \n",
            "In measure and order, the cosmos align,  \n",
            "A divine reason, poetic and divine,  \n",
            "So grasp your mind, let logic be your guide,  \n",
            "That’s the true meaning—wisdom deep inside.  \n",
            "  \n",
            "So walk with purpose, with a rational mind,  \n",
            "In pursuit of truth, eternal we find,  \n",
            "That life’s most profound, in the soul’s own quest,  \n",
            "Is to reason, to love—be your very best.\u001b[0m \n"
          ]
        }
      ],
      "source": [
        "role = \"17th century rationalist philosopher\"\n",
        "style = \"You have a very deep and complex knowledge of philosophy and you are able to explain it in a way that is easy to understand.\"\n",
        "format_ = \"You answer should be a rap song.\"\n",
        "question = \"what is the meaning of life?\"\n",
        "\n",
        "prompt = (\n",
        "    f\"\"\"You are a {role}. \n",
        "    Your task is to answer the following question in the specified style and format.\\n\\n\n",
        "    Style: {style}\\n\n",
        "    Format: {format_}\\n\\n\n",
        "    Question: {question}\"\"\"\n",
        ")\n",
        "response = llm.invoke(prompt).content\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "prompt:\n",
            "\u001b[31mPrompt: \n",
            "You are a 17th century rationalist philosopher. \n",
            "    Your task is to answer the following question in the specified style and format.\n",
            "\n",
            "\n",
            "    Style: You have a very deep and complex knowledge of philosophy and you are able to explain it in a way that is easy to understand.\n",
            "\n",
            "    Format: You answer should be a rap song.\n",
            "\n",
            "\n",
            "    Question: what is the meaning of life?\u001b[0m\n",
            "\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\u001b[34mResponse:\n",
            " Yo, listen up, I’ma break it down deep,   The meaning of life, it ain’t just to\n",
            "sleep,   It’s that spark in the soul, that rational fire,   To seek truth and\n",
            "knowledge, to lift us higher.      From Descartes’ mind, I think, therefore I\n",
            "am,   Our essence’s reason, that’s who I am,   We question, explore, through\n",
            "doubt we refine,   Finding the divine in the reasoned design.      Life’s about\n",
            "the quest, the pursuit of the good,   Living with virtue, doing what we should,\n",
            "Not just fleeting pleasures or superficial gains,   But eternal truths that\n",
            "lessen our pains.      In measure and order, the cosmos align,   A divine\n",
            "reason, poetic and divine,   So grasp your mind, let logic be your guide,\n",
            "That’s the true meaning—wisdom deep inside.      So walk with purpose, with a\n",
            "rational mind,   In pursuit of truth, eternal we find,   That life’s most\n",
            "profound, in the soul’s own quest,   Is to reason, to love—be your very best.\u001b[0m \n"
          ]
        }
      ],
      "source": [
        "print(\"prompt:\")\n",
        "print(f\"\\033[31mPrompt: \\n{prompt}\\033[0m\")\n",
        "print(\"\")\n",
        "print(\"-\"*100)\n",
        "import textwrap\n",
        "wrapped_response = textwrap.fill(response, width=80)\n",
        "print(f\"\\033[34mResponse:\\n {wrapped_response}\\033[0m \")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def euclidean_distance(x: float, y: float) -> float:\n",
        "   dist = np.sqrt(np.sum((x - y) ** 2))\n",
        "   return dist\n",
        "\n",
        "\n",
        "def cosine_similarity(x: float, y: float) -> float:\n",
        "    return np.dot(x, y) / (np.linalg.norm(x) * np.linalg.norm(y))\n",
        "    #return cos_sim\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "review = \"\"\"\n",
        "לא אכלתי. ראתי את מחירי הפיצות וניבהלתי. 80 שח לפיצה אישית זה מוגזם.\n",
        "תפריט יש רק דיגיטלי בסריקת קוד, קשישים ואוטיסטים רבים יתקשו להזמין שם.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "prompt = f\"\"\"Perform a sentiment analysis on the following restaurant review into 3 sentiment groups: positive, neutral, and negative.\n",
        "\n",
        "Review: {review}\n",
        "\n",
        "Examples:\n",
        "Review 1: \"הזמננו דרך וולט. לקח הרבה זמן. הפיצה היתה קרה כמו סוליה של מגף. הפסטות מוצפות בשמן, שמנות נורא, לא מורגש שום טעם חוץ משל שמן זול. בטוח לא הטעם של כמהין שהיה מובטח.\n",
        "פסטה מבושלת יותר מדי, לא al dente.\n",
        "אכזבה גדולה. מצטער שהתעצלתי ולא טגנתי במקום ההזמנה הזאת איזה שניצל או לא בישלתי פסטה נורמלית טעימה בעצמי.\n",
        "פעם ראשונה ואחרונה.\n",
        "לא מומלץ בשום אופן.\"\n",
        "Sentiment: negative\n",
        "\n",
        "Review 2: \" פיצה בינונית מאוד, נמכרת במחיר 80-90 שקל למגש כאילו זו פיצת שף,\n",
        "הבצק דחוס ובקשוי תפח, והגבינה עלובה (בטח איזשהי גבינה שקונים מגורדת)\n",
        "אם הייתה לי בחינת טעימה עיוורת, הייתי מנחש שמדובר בפיצה ב- 36שח למגש בתחנה מרכזית\"\n",
        "Sentiment: neutral\n",
        "\n",
        "Review 3: \"עדיין הפיצה הכי טובה בעיר. פיצה איכותית, עשירה, תפריט מדליק.\n",
        "ממליץ על פיצה אה-לה-רומנה או פפרוני.\n",
        "הבצק קריספי בטירוף ורואים שהושקעה מחשבה בהגשה. הישיבה ברחוב המלבן פינת כצנלסון נעימה וכיפית.\n",
        "\n",
        "תמיד נעים לחזור - תודה לכם צוות פאצה פיצה.\"\n",
        "Sentiment: positive\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "response = gpt4o_chat.invoke(prompt).content\n",
        "\n",
        "\n",
        "print(f\"\\033[31mPrompt: {prompt}\\033[0m\")\n",
        "print(\"\")\n",
        "print(f\"\\033[34mResponse: {response}\\033[0m \")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
