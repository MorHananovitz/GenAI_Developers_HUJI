{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1_kb3erex7Cq"
      },
      "source": [
        "# Introducion to GenAI\n",
        "\n",
        "Mor Hananovitz (c)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install langchain_openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "VmCkkGWkyZ-o"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/mor.h/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "# Load environment variables (you'll need to create a .env file with your OpenAI API key)\n",
        "load_dotenv()\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Prompt Engineering 101"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "-OmyN8ntBQ6U"
      },
      "outputs": [],
      "source": [
        "gpt4o_chat = ChatOpenAI(model=\"gpt-4o-2024-08-06\", temperature=0, max_tokens=1000)\n",
        "gpt35_chat = ChatOpenAI(model=\"gpt-3.5-turbo-0125\", temperature=0, max_tokens=1000)\n",
        "gpt_4o_mini_chat = ChatOpenAI(model=\"o4-mini-2025-04-16\", max_tokens=1000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Available OpenAI Models:\n",
            "==================================================\n",
            "\n",
            "gpt-4o\n",
            "  Type: text_generation\n",
            "  Description: Most capable GPT model, multimodal (text + images)\n",
            "  Context Window: 128,000 tokens\n",
            "  Input Price: $2.5/1M tokens\n",
            "  Output Price: $10.0/1M tokens\n",
            "\n",
            "gpt-4o-mini\n",
            "  Type: text_generation\n",
            "  Description: Faster, cheaper version of GPT-4o\n",
            "  Context Window: 128,000 tokens\n",
            "  Input Price: $0.15/1M tokens\n",
            "  Output Price: $0.6/1M tokens\n",
            "\n",
            "gpt-4.1\n",
            "  Type: text_generation\n",
            "  Description: Latest GPT model with improved capabilities\n",
            "  Context Window: 200,000 tokens\n",
            "  Input Price: $10.0/1M tokens\n",
            "  Output Price: $30.0/1M tokens\n",
            "\n",
            "gpt-4.1-mini\n",
            "  Type: text_generation\n",
            "  Description: Smaller, faster version of GPT-4.1\n",
            "  Context Window: 200,000 tokens\n",
            "  Input Price: $1.0/1M tokens\n",
            "  Output Price: $4.0/1M tokens\n",
            "\n",
            "gpt-4.1-nano\n",
            "  Type: text_generation\n",
            "  Description: Ultra-fast, cost-effective model\n",
            "  Context Window: 200,000 tokens\n",
            "  Input Price: $0.25/1M tokens\n",
            "  Output Price: $1.0/1M tokens\n",
            "\n",
            "gpt-3.5-turbo\n",
            "  Type: text_generation\n",
            "  Description: Legacy model, still capable for many tasks\n",
            "  Context Window: 16,385 tokens\n",
            "  Input Price: $0.5/1M tokens\n",
            "  Output Price: $1.5/1M tokens\n",
            "\n",
            "o3\n",
            "  Type: reasoning\n",
            "  Description: Most advanced reasoning model for complex problems\n",
            "  Context Window: 200,000 tokens\n",
            "  Input Price: $60.0/1M tokens\n",
            "  Output Price: $240.0/1M tokens\n",
            "\n",
            "o4-mini\n",
            "  Type: reasoning\n",
            "  Description: Powerful reasoning model, more affordable than o3\n",
            "  Context Window: 200,000 tokens\n",
            "  Input Price: $1.1/1M tokens\n",
            "  Output Price: $4.4/1M tokens\n",
            "\n",
            "whisper-1\n",
            "  Type: audio_transcription\n",
            "  Description: Speech-to-text transcription and translation\n",
            "  Price: $0.006/minute\n",
            "\n",
            "dall-e-3\n",
            "  Type: image_generation\n",
            "  Description: High-quality image generation\n",
            "\n",
            "gpt-image-1\n",
            "  Type: image_generation\n",
            "  Description: Latest image generation model, successor to DALL-E\n",
            "  Input Price: $5.0/1M tokens\n",
            "  Output Price: $40.0/1M tokens\n",
            "\n",
            "text-embedding-3-large\n",
            "  Type: embeddings\n",
            "  Description: Most capable embedding model\n",
            "\n",
            "text-embedding-3-small\n",
            "  Type: embeddings\n",
            "  Description: Efficient embedding model\n",
            "\n",
            "text-embedding-ada-002\n",
            "  Type: embeddings\n",
            "  Description: Legacy embedding model\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Available OpenAI Models (as of 2025)\n",
        "available_models = {\n",
        "    # GPT Models - Text Generation\n",
        "    \"gpt-4o\": {\n",
        "        \"type\": \"text_generation\",\n",
        "        \"description\": \"Most capable GPT model, multimodal (text + images)\",\n",
        "        \"context_window\": 128000,\n",
        "        \"input_price_per_1m_tokens\": 2.50,\n",
        "        \"output_price_per_1m_tokens\": 10.00\n",
        "    },\n",
        "    \"gpt-4o-mini\": {\n",
        "        \"type\": \"text_generation\", \n",
        "        \"description\": \"Faster, cheaper version of GPT-4o\",\n",
        "        \"context_window\": 128000,\n",
        "        \"input_price_per_1m_tokens\": 0.15,\n",
        "        \"output_price_per_1m_tokens\": 0.60\n",
        "    },\n",
        "    \"gpt-4.1\": {\n",
        "        \"type\": \"text_generation\",\n",
        "        \"description\": \"Latest GPT model with improved capabilities\",\n",
        "        \"context_window\": 200000,\n",
        "        \"input_price_per_1m_tokens\": 10.00,\n",
        "        \"output_price_per_1m_tokens\": 30.00\n",
        "    },\n",
        "    \"gpt-4.1-mini\": {\n",
        "        \"type\": \"text_generation\",\n",
        "        \"description\": \"Smaller, faster version of GPT-4.1\",\n",
        "        \"context_window\": 200000,\n",
        "        \"input_price_per_1m_tokens\": 1.00,\n",
        "        \"output_price_per_1m_tokens\": 4.00\n",
        "    },\n",
        "    \"gpt-4.1-nano\": {\n",
        "        \"type\": \"text_generation\",\n",
        "        \"description\": \"Ultra-fast, cost-effective model\",\n",
        "        \"context_window\": 200000,\n",
        "        \"input_price_per_1m_tokens\": 0.25,\n",
        "        \"output_price_per_1m_tokens\": 1.00\n",
        "    },\n",
        "    \"gpt-3.5-turbo\": {\n",
        "        \"type\": \"text_generation\",\n",
        "        \"description\": \"Legacy model, still capable for many tasks\",\n",
        "        \"context_window\": 16385,\n",
        "        \"input_price_per_1m_tokens\": 0.50,\n",
        "        \"output_price_per_1m_tokens\": 1.50\n",
        "    },\n",
        "    \n",
        "    # Reasoning Models\n",
        "    \"o3\": {\n",
        "        \"type\": \"reasoning\",\n",
        "        \"description\": \"Most advanced reasoning model for complex problems\",\n",
        "        \"context_window\": 200000,\n",
        "        \"input_price_per_1m_tokens\": 60.00,\n",
        "        \"output_price_per_1m_tokens\": 240.00\n",
        "    },\n",
        "    \"o4-mini\": {\n",
        "        \"type\": \"reasoning\",\n",
        "        \"description\": \"Powerful reasoning model, more affordable than o3\",\n",
        "        \"context_window\": 200000,\n",
        "        \"input_price_per_1m_tokens\": 1.10,\n",
        "        \"output_price_per_1m_tokens\": 4.40\n",
        "    },\n",
        "    \n",
        "    # Audio Models\n",
        "    \"whisper-1\": {\n",
        "        \"type\": \"audio_transcription\",\n",
        "        \"description\": \"Speech-to-text transcription and translation\",\n",
        "        \"price_per_minute\": 0.006\n",
        "    },\n",
        "    \n",
        "    # Image Generation Models\n",
        "    \"dall-e-3\": {\n",
        "        \"type\": \"image_generation\",\n",
        "        \"description\": \"High-quality image generation\",\n",
        "        \"price_1024x1024_standard\": 0.040,\n",
        "        \"price_1024x1024_hd\": 0.080\n",
        "    },\n",
        "    \"gpt-image-1\": {\n",
        "        \"type\": \"image_generation\",\n",
        "        \"description\": \"Latest image generation model, successor to DALL-E\",\n",
        "        \"input_price_per_1m_tokens\": 5.00,\n",
        "        \"output_price_per_1m_tokens\": 40.00\n",
        "    },\n",
        "    \n",
        "    # Embedding Models\n",
        "    \"text-embedding-3-large\": {\n",
        "        \"type\": \"embeddings\",\n",
        "        \"description\": \"Most capable embedding model\",\n",
        "        \"dimensions\": 3072,\n",
        "        \"price_per_1m_tokens\": 0.13\n",
        "    },\n",
        "    \"text-embedding-3-small\": {\n",
        "        \"type\": \"embeddings\", \n",
        "        \"description\": \"Efficient embedding model\",\n",
        "        \"dimensions\": 1536,\n",
        "        \"price_per_1m_tokens\": 0.02\n",
        "    },\n",
        "    \"text-embedding-ada-002\": {\n",
        "        \"type\": \"embeddings\",\n",
        "        \"description\": \"Legacy embedding model\",\n",
        "        \"dimensions\": 1536,\n",
        "        \"price_per_1m_tokens\": 0.10\n",
        "    }\n",
        "}\n",
        "\n",
        "print(\"Available OpenAI Models:\")\n",
        "print(\"=\" * 50)\n",
        "for model_name, details in available_models.items():\n",
        "    print(f\"\\n{model_name}\")\n",
        "    print(f\"  Type: {details['type']}\")\n",
        "    print(f\"  Description: {details['description']}\")\n",
        "    if 'context_window' in details:\n",
        "        print(f\"  Context Window: {details['context_window']:,} tokens\")\n",
        "    if 'input_price_per_1m_tokens' in details:\n",
        "        print(f\"  Input Price: ${details['input_price_per_1m_tokens']}/1M tokens\")\n",
        "    if 'output_price_per_1m_tokens' in details:\n",
        "        print(f\"  Output Price: ${details['output_price_per_1m_tokens']}/1M tokens\")\n",
        "    if 'price_per_minute' in details:\n",
        "        print(f\"  Price: ${details['price_per_minute']}/minute\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P4WFdQo4BkuH"
      },
      "source": [
        "### Prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "21wBbc57Wk5o",
        "outputId": "7df278cc-72d9-4ad6-ac64-64a5eddcef5d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[31mPrompt: hi what is your name?\u001b[0m\n",
            "\n",
            "\u001b[34mResponse: Hello! I am a language model AI assistant and I do not have a personal name. You can just call me Assistant. How can I help you today?\u001b[0m \n"
          ]
        }
      ],
      "source": [
        "prompt = \"\"\"hi what is your name?\"\"\"\n",
        "\n",
        "\n",
        "response = gpt35_chat.invoke(prompt).content\n",
        "#response = gpt_4o_mini_chat.invoke(prompt).content\n",
        "\n",
        "\n",
        "print(f\"\\033[31mPrompt: {prompt}\\033[0m\")\n",
        "print(\"\")\n",
        "print(f\"\\033[34mResponse: {response}\\033[0m \")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Shots Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GvjKy03Ff1og"
      },
      "source": [
        "Zero-Shot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0v89XvUQ0hVo",
        "outputId": "ac7a88b1-5bde-475e-9937-d2a0a4a0ed38"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[31mPrompt: Classify the following food items into three groups: A, B, and C.\n",
            "\n",
            "Items: Carrot, Chicken, Spinach, Beef, Banana, Lettuce, Pork, Milk, Butter\u001b[0m\n",
            "\n",
            "\u001b[34mResponse: Group A: Carrot, Spinach, Banana, Lettuce, Milk\n",
            "Group B: Chicken, Beef, Pork\n",
            "Group C: Butter\u001b[0m \n"
          ]
        }
      ],
      "source": [
        "prompt = \"\"\"Classify the following food items into three groups: A, B, and C.\n",
        "\n",
        "Items: Carrot, Chicken, Spinach, Beef, Banana, Lettuce, Pork, Milk, Butter\"\"\"\n",
        "\n",
        "response = gpt35_chat.invoke(prompt).content\n",
        "\n",
        "#response = gpt4o_chat.invoke(prompt).content\n",
        "\n",
        "\n",
        "print(f\"\\033[31mPrompt: {prompt}\\033[0m\")\n",
        "print(\"\")\n",
        "print(f\"\\033[34mResponse: {response}\\033[0m \")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[31mPrompt: Classify the following food items into three groups: A, B, and C.\n",
            "\n",
            "Items: Carrot, Chicken, Spinach, Beef, Banana, Lettuce, Pork, Milk, Butter\u001b[0m\n",
            "\n",
            "\u001b[34mResponse: Here is one way to classify the food items into three groups:\n",
            "\n",
            "Group A (Vegetables):\n",
            "- Carrot\n",
            "- Spinach\n",
            "- Lettuce\n",
            "\n",
            "Group B (Meats):\n",
            "- Chicken\n",
            "- Beef\n",
            "- Pork\n",
            "\n",
            "Group C (Dairy and Fruits):\n",
            "- Banana\n",
            "- Milk\n",
            "- Butter\u001b[0m \n"
          ]
        }
      ],
      "source": [
        "prompt = \"\"\"Classify the following food items into three groups: A, B, and C.\n",
        "\n",
        "Items: Carrot, Chicken, Spinach, Beef, Banana, Lettuce, Pork, Milk, Butter\"\"\"\n",
        "\n",
        "response = gpt4o_chat.invoke(prompt).content\n",
        "\n",
        "\n",
        "print(f\"\\033[31mPrompt: {prompt}\\033[0m\")\n",
        "print(\"\")\n",
        "print(f\"\\033[34mResponse: {response}\\033[0m \")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pwaybKUUgGff"
      },
      "source": [
        "One-Shot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rZitniNgs12_",
        "outputId": "ea973e06-9731-430c-8665-09ca94e0529c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[31mPrompt: Classify the following food items into three groups: A, B, and C.\n",
            "\n",
            "Example:\n",
            "Apple → A\n",
            "\n",
            "Items: Carrot, Chicken, Spinach, Beef, Banana, Lettuce, Pork, Milk, Butter\u001b[0m\n",
            "\n",
            "\u001b[34mResponse: Carrot, Spinach, Lettuce → A\n",
            "Chicken, Beef, Pork → B\n",
            "Banana, Milk, Butter → C\u001b[0m \n"
          ]
        }
      ],
      "source": [
        "prompt = \"\"\"Classify the following food items into three groups: A, B, and C.\n",
        "\n",
        "Example:\n",
        "Apple → A\n",
        "\n",
        "Items: Carrot, Chicken, Spinach, Beef, Banana, Lettuce, Pork, Milk, Butter\"\"\"\n",
        "\n",
        "\n",
        "response = gpt35_chat.invoke(prompt).content\n",
        "\n",
        "#response = gpt4o_chat.invoke(prompt).content\n",
        "\n",
        "print(f\"\\033[31mPrompt: {prompt}\\033[0m\")\n",
        "print(\"\")\n",
        "print(f\"\\033[34mResponse: {response}\\033[0m \")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[31mPrompt: Classify the following food items into three groups: A, B, and C.\n",
            "\n",
            "Example:\n",
            "Apple → A\n",
            "\n",
            "Items: Carrot, Chicken, Spinach, Beef, Banana, Lettuce, Pork, Milk, Butter\u001b[0m\n",
            "\n",
            "\u001b[34mResponse: To classify the food items into three groups, we can use the following criteria:\n",
            "\n",
            "- Group A: Fruits and vegetables\n",
            "- Group B: Meats\n",
            "- Group C: Dairy products\n",
            "\n",
            "Using these criteria, the classification would be:\n",
            "\n",
            "- Carrot → A\n",
            "- Chicken → B\n",
            "- Spinach → A\n",
            "- Beef → B\n",
            "- Banana → A\n",
            "- Lettuce → A\n",
            "- Pork → B\n",
            "- Milk → C\n",
            "- Butter → C\u001b[0m \n"
          ]
        }
      ],
      "source": [
        "prompt = \"\"\"Classify the following food items into three groups: A, B, and C.\n",
        "\n",
        "Example:\n",
        "Apple → A\n",
        "\n",
        "Items: Carrot, Chicken, Spinach, Beef, Banana, Lettuce, Pork, Milk, Butter\"\"\"\n",
        "\n",
        "\n",
        "response = gpt4o_chat.invoke(prompt).content\n",
        "\n",
        "print(f\"\\033[31mPrompt: {prompt}\\033[0m\")\n",
        "print(\"\")\n",
        "print(f\"\\033[34mResponse: {response}\\033[0m \")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5f7iDJe3gPfu"
      },
      "source": [
        "Few-Shot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IUxQZ1pkgPl0",
        "outputId": "16463899-e7af-475d-ff93-7693e1c06d76"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[31mPrompt: Classify the following food items into three groups: A, B, and C.\n",
            "Items: Spinach, Lettuce, Pork, Cheese, Yogurt, Pinnaple, Whip cream, Peach, eggs\n",
            "\n",
            "Examples:\n",
            "Apple → A\n",
            "Carrot → A\n",
            "Chicken → B\n",
            "Banana → A\n",
            "Milk - C\n",
            "Butter - C\n",
            "\n",
            "\u001b[0m\n",
            "\n",
            "\u001b[34mResponse: Group A: Spinach, Lettuce, Pineapple, Peach, Eggs\n",
            "Group B: Pork\n",
            "Group C: Cheese, Yogurt, Whip cream\u001b[0m \n"
          ]
        }
      ],
      "source": [
        "prompt = \"\"\"Classify the following food items into three groups: A, B, and C.\n",
        "Items: Spinach, Lettuce, Pork, Cheese, Yogurt, Pinnaple, Whip cream, Peach, eggs\n",
        "\n",
        "Examples:\n",
        "Apple → A\n",
        "Carrot → A\n",
        "Chicken → B\n",
        "Banana → A\n",
        "Milk - C\n",
        "Butter - C\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "response = gpt35_chat.invoke(prompt).content\n",
        "\n",
        "#response = gpt4o_chat.invoke(prompt).content\n",
        "\n",
        "\n",
        "print(f\"\\033[31mPrompt: {prompt}\\033[0m\")\n",
        "print(\"\")\n",
        "print(f\"\\033[34mResponse: {response}\\033[0m \")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[31mPrompt: Classify the following food items into three groups: A, B, and C.\n",
            "Items: Spinach, Lettuce, Pork, Cheese, Yogurt, Pinnaple, Whip cream, Peach, eggs\n",
            "\n",
            "Examples:\n",
            "Apple → A\n",
            "Carrot → A\n",
            "Chicken → B\n",
            "Banana → A\n",
            "Milk - C\n",
            "Butter - C\n",
            "\n",
            "Can you also label the groups?\n",
            "\u001b[0m\n",
            "\n",
            "\u001b[34mResponse: Certainly! Based on the examples provided, here is the classification of the food items into groups A, B, and C:\n",
            "\n",
            "- **Group A (Fruits and Vegetables):**\n",
            "  - Spinach\n",
            "  - Lettuce\n",
            "  - Pineapple\n",
            "  - Peach\n",
            "\n",
            "- **Group B (Meat and Protein):**\n",
            "  - Pork\n",
            "  - Eggs\n",
            "\n",
            "- **Group C (Dairy and Dairy Products):**\n",
            "  - Cheese\n",
            "  - Yogurt\n",
            "  - Whip cream\n",
            "\n",
            "These classifications are based on the nature of the items and their similarity to the examples given.\u001b[0m \n"
          ]
        }
      ],
      "source": [
        "prompt = \"\"\"Classify the following food items into three groups: A, B, and C.\n",
        "Items: Spinach, Lettuce, Pork, Cheese, Yogurt, Pinnaple, Whip cream, Peach, eggs\n",
        "\n",
        "Examples:\n",
        "Apple → A\n",
        "Carrot → A\n",
        "Chicken → B\n",
        "Banana → A\n",
        "Milk - C\n",
        "Butter - C\n",
        "\n",
        "Can you also label the groups?\n",
        "\"\"\"\n",
        "\n",
        "response = gpt4o_chat.invoke(prompt).content\n",
        "\n",
        "\n",
        "print(f\"\\033[31mPrompt: {prompt}\\033[0m\")\n",
        "print(\"\")\n",
        "print(f\"\\033[34mResponse: {response}\\033[0m \")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Response: Certainly! Based on the examples provided, here is the classification of the food items into groups A, B, and C:\n",
        "\n",
        "- **Group A (Fruits and Vegetables):**\n",
        "  - Spinach\n",
        "  - Lettuce\n",
        "  - Pineapple\n",
        "  - Peach\n",
        "\n",
        "- **Group B (Meat and Protein Sources):**\n",
        "  - Pork\n",
        "  - Eggs\n",
        "\n",
        "- **Group C (Dairy Products):**\n",
        "  - Cheese\n",
        "  - Yogurt\n",
        "  - Whip cream\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LLHAAClSicNa"
      },
      "source": [
        "### Chain of Thought Prompting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VORxDF2Vicde",
        "outputId": "5d4820b9-579c-4ba8-80c7-2f3ac73d3c6e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[31mPrompt: If Sarah had 8 pencils, gave 3 to her friend, then bought 6 more pencils, but 2 of the new pencils were broken, how many pencils does Sarah have now?\n",
            "\n",
            "\u001b[0m\n",
            "\n",
            "\u001b[34mResponse: Sarah initially had 8 pencils. She gave 3 to her friend, leaving her with 8 - 3 = 5 pencils. She then bought 6 more pencils, bringing her total to 5 + 6 = 11 pencils. However, 2 of the new pencils were broken, so she effectively has 11 - 2 = 9 usable pencils. Therefore, Sarah has 9 pencils now.\u001b[0m \n"
          ]
        }
      ],
      "source": [
        "prompt = \"\"\"If Sarah had 8 pencils, gave 3 to her friend, then bought 6 more pencils, but 2 of the new pencils were broken, how many pencils does Sarah have now?\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "#response = gpt35_chat.invoke(prompt).content\n",
        "\n",
        "response = gpt4o_chat.invoke(prompt).content\n",
        "\n",
        "\n",
        "print(f\"\\033[31mPrompt: {prompt}\\033[0m\")\n",
        "print(\"\")\n",
        "print(f\"\\033[34mResponse: {response}\\033[0m \")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vdj8xUZFjzOu",
        "outputId": "581e0fe3-b444-40d8-eb42-c97e55938f8a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[31mPrompt: If Sarah had 8 pencils, gave 3 to her friend, then bought 6 more pencils, but 2 of the new pencils were broken, how many pencils does Sarah have now?\n",
            "\n",
            "Let's break this down step by step.\n",
            "\n",
            "\n",
            "\n",
            "\u001b[0m\n",
            "\n",
            "\u001b[34mResponse: 1. Sarah starts with 8 pencils.\n",
            "2. She gives 3 pencils to her friend, so she now has 8 - 3 = 5 pencils.\n",
            "3. She buys 6 more pencils, so she now has 5 + 6 = 11 pencils.\n",
            "4. However, 2 of the new pencils were broken, so she now has 11 - 2 = 9 pencils.\n",
            "\n",
            "Therefore, Sarah now has 9 pencils.\u001b[0m \n"
          ]
        }
      ],
      "source": [
        "prompt = \"\"\"If Sarah had 8 pencils, gave 3 to her friend, then bought 6 more pencils, but 2 of the new pencils were broken, how many pencils does Sarah have now?\n",
        "\n",
        "Let's break this down step by step.\n",
        "\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "response = gpt35_chat.invoke(prompt).content\n",
        "\n",
        "#response = gpt4o_chat.invoke(prompt).content\n",
        "\n",
        "\n",
        "print(f\"\\033[31mPrompt: {prompt}\\033[0m\")\n",
        "print(\"\")\n",
        "print(f\"\\033[34mResponse: {response}\\033[0m \")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Prompt Format Example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {},
      "outputs": [],
      "source": [
        "context = \"\"\" I am happy\"\"\"\n",
        "\n",
        "example  = \"\"\"I love rainbows\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[31mPrompt: \n",
            "can you tell me what is the sentiment of this  I am happy?\n",
            "\n",
            "For example:\n",
            "review: I love rainbows\n",
            "sentiment: positive\n",
            "\n",
            "Output format:\n",
            "sentiment: positive or negative\n",
            "\u001b[0m\n",
            "\n",
            "\u001b[34mResponse: sentiment: positive\u001b[0m \n"
          ]
        }
      ],
      "source": [
        "prompt = f\"\"\"\n",
        "can you tell me what is the sentiment of this {context}?\n",
        "\n",
        "For example:\n",
        "review: {example}\n",
        "sentiment: positive\n",
        "\n",
        "Output format:\n",
        "sentiment: positive or negative\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "response = gpt_4o_mini_chat.invoke(prompt).content\n",
        "\n",
        "\n",
        "\n",
        "print(f\"\\033[31mPrompt: {prompt}\\033[0m\")\n",
        "print(\"\")\n",
        "print(f\"\\033[34mResponse: {response}\\033[0m \")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def euclidean_distance(x: float, y: float) -> float:\n",
        "   dist = np.sqrt(np.sum((x - y) ** 2))\n",
        "   return dist\n",
        "\n",
        "\n",
        "def cosine_similarity(x: float, y: float) -> float:\n",
        "    return np.dot(x, y) / (np.linalg.norm(x) * np.linalg.norm(y))\n",
        "    #return cos_sim\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
