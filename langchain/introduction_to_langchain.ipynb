{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# ğŸš€ Introduction to LangChain for Beginners\n",
    "\n",
    "Welcome to your first steps into the world of LangChain! This notebook will guide you through the fundamentals of LangChain, a powerful framework for building applications with Large Language Models (LLMs).\n",
    "\n",
    "## ğŸ“š What You'll Learn\n",
    "\n",
    "1. **What is LangChain?** - Understanding the basics\n",
    "2. **Core Components** - LLMs, Prompts, Chains, and more\n",
    "3. **Building Your First Chain** - Hands-on examples\n",
    "4. **Working with Prompts** - Creating effective prompts\n",
    "5. **Memory and Context** - Maintaining conversation history\n",
    "6. **Real-world Applications** - Practical examples\n",
    "\n",
    "## ğŸ¯ Prerequisites\n",
    "\n",
    "Before starting, make sure you have:\n",
    "- Basic Python knowledge\n",
    "- Understanding of what LLMs are\n",
    "- Curiosity and enthusiasm! ğŸ‰\n",
    "\n",
    "Let's begin our journey! ğŸš€\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## ğŸ“¦ Installation and Setup\n",
    "\n",
    "First, let's install the required packages. Run the cell below:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install --quiet langchain-openai langchain-community langchain-core python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mor.h/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… All imports successful!\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain, SimpleSequentialChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.schema import HumanMessage, SystemMessage\n",
    "\n",
    "# Load environment variables (you'll need to create a .env file with your OpenAI API key)\n",
    "load_dotenv()\n",
    "\n",
    "print(\"âœ… All imports successful!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/4k/bd53pxq144v00900ycf02bch0000gp/T/ipykernel_73327/939706259.py:5: LangChainDeprecationWarning: The class `OpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import OpenAI``.\n",
      "  llm = OpenAI(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ‰ LLM initialized with all parameters!\n",
      "Model: gpt-3.5-turbo-instruct\n",
      "Temperature: 0.7\n",
      "Max tokens: 256\n"
     ]
    }
   ],
   "source": [
    "# Set up your OpenAI API key (make sure you have it in your .env file)\n",
    "# Create a .env file in your project directory with: OPENAI_API_KEY=your_api_key_here\n",
    "\n",
    "# Initialize the OpenAI LLM with all possible parameters\n",
    "llm_example = OpenAI(\n",
    "    # Core parameters\n",
    "    model_name=\"gpt-3.5-turbo-instruct\",  # The specific model to use (text-davinci-003, gpt-3.5-turbo-instruct, etc.)\n",
    "    temperature=0.7,                       # Controls randomness (0.0 = deterministic, 1.0 = very creative)\n",
    "    max_tokens=256,                        # Maximum number of tokens to generate in the response\n",
    "    top_p=1.0,                            # Nucleus sampling - considers tokens with top_p probability mass\n",
    "    frequency_penalty=0.0,                 # Penalizes frequent tokens (-2.0 to 2.0, reduces repetition)\n",
    "    presence_penalty=0.0,                  # Penalizes new tokens based on presence (-2.0 to 2.0, encourages new topics)\n",
    "    n=1,                                   # Number of completions to generate for each prompt\n",
    "    best_of=1,                            # Generates best_of completions server-side, returns the best one\n",
    "    \n",
    "    # Advanced parameters\n",
    "    logit_bias={},                        # Modify likelihood of specified tokens (token_id: bias_value)\n",
    "    openai_api_key=os.getenv(\"OPENAI_API_KEY\"),  # Your OpenAI API key\n",
    "    openai_organization=None,             # OpenAI organization ID (if applicable)\n",
    "    batch_size=20,                        # Batch size for requests when processing multiple prompts\n",
    "    request_timeout=60,                   # Timeout for API requests in seconds\n",
    "    max_retries=6,                        # Maximum number of retries for failed requests\n",
    "    streaming=False,                      # Whether to stream the response\n",
    "    allowed_special=\"all\",                # Which special tokens are allowed in the input\n",
    "    disallowed_special=\"all\",             # Which special tokens are disallowed in the input\n",
    "    \n",
    "    # Caching and performance\n",
    "    cache=None,                           # Cache implementation for storing results\n",
    "    verbose=False,                        # Whether to print verbose output\n",
    "    callbacks=None,                       # List of callback handlers\n",
    "    callback_manager=None,                # Callback manager for handling callbacks\n",
    "    tags=None,                           # Tags for tracking and organizing requests\n",
    "    metadata=None,                       # Additional metadata for the LLM\n",
    ")\n",
    "\n",
    "print(\"ğŸ‰ LLM initialized with all parameters!\")\n",
    "print(f\"Model: {llm.model_name}\")\n",
    "print(f\"Temperature: {llm.temperature}\")\n",
    "print(f\"Max tokens: {llm.max_tokens}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/4k/bd53pxq144v00900ycf02bch0000gp/T/ipykernel_83899/648172774.py:1: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import ChatOpenAI``.\n",
      "  llm = ChatOpenAI(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ‰ LLM initialized with all parameters!\n",
      "Model: gpt-4.1-nano-2025-04-14\n",
      "Temperature: 0.0\n",
      "Max tokens: 256\n"
     ]
    }
   ],
   "source": [
    "llm = ChatOpenAI(\n",
    "    model_name=\"gpt-4.1-nano-2025-04-14\",  \n",
    "    temperature=0,                       \n",
    "    max_tokens=256)\n",
    "\n",
    "print(\"ğŸ‰ LLM initialized with all parameters!\")\n",
    "print(f\"Model: {llm.model_name}\")\n",
    "print(f\"Temperature: {llm.temperature}\")\n",
    "print(f\"Max tokens: {llm.max_tokens}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## ğŸ¤” What is LangChain?\n",
    "\n",
    "**LangChain** is like a toolkit for building AI applications. Think of it as LEGO blocks for AI! ğŸ§±\n",
    "\n",
    "### Why LangChain?\n",
    "\n",
    "1. **Modularity**: Build complex AI applications from simple components\n",
    "2. **Flexibility**: Work with different LLM providers (OpenAI, Anthropic, etc.)\n",
    "3. **Memory**: Maintain context across conversations\n",
    "4. **Chains**: Connect multiple AI operations together\n",
    "5. **Tools**: Integrate with external data sources and APIs\n",
    "\n",
    "### ğŸ”§ Core Concepts\n",
    "\n",
    "| Component | Description | Example |\n",
    "|-----------|-------------|---------| \n",
    "| **LLMs** | Language models that generate text | GPT-3.5, GPT-4 |\n",
    "| **Prompts** | Instructions for the LLM | \"Explain quantum physics in simple terms\" |\n",
    "| **Chains** | Sequences of operations | Question â†’ Answer â†’ Summary |\n",
    "| **Memory** | Storing conversation history | Remembering previous questions |\n",
    "| **Tools** | External integrations | Web search, calculator, database |\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ¯ Quick Question\n",
    "\n",
    "**Think about this**: If you wanted to build a chatbot that can remember your name and preferences, which LangChain components would you need?\n",
    "\n",
    "Write your answer below:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "**Your Answer:** [Write your thoughts here]\n",
    "\n",
    "**Hint**: Consider what components would handle the conversation, store information, and generate responses.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## ğŸ¯ Section 1: Your First LangChain Application\n",
    "\n",
    "Let's start with the simplest possible example - a basic text generator!\n",
    "\n",
    "`llm = OpenAI(temperature=0.7)`\n",
    "\n",
    "Above line of code in this notebook imports the OpenAI class from `langchain.llms` and creates an instance without specifying a particular model name. \n",
    "When no model is explicitly specified, the OpenAI class in LangChain <u>defaults</u> to using \"text-davinci-003\" (which is **GPT-3.5**).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¤– LLM Response:\n",
      "Why do programmers prefer dark mode? Because light attracts bugs!\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Initialize the LLM\n",
    "# Note: You'll need an OpenAI API key in your .env file\n",
    "# Create a .env file with: OPENAI_API_KEY=your_api_key_here\n",
    "\n",
    "try:\n",
    "    # Initialize the language model\n",
    "    llm = ChatOpenAI(\n",
    "    model_name=\"gpt-4.1-nano-2025-04-14\",  \n",
    "    temperature=0,                       \n",
    "    max_tokens=256)  \n",
    "    \n",
    "    # Test the LLM\n",
    "    response = llm.predict(\"Tell me a short joke about programming\")\n",
    "    print(\"ğŸ¤– LLM Response:\")\n",
    "    print(response)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ Error: {e}\")\n",
    "    print(\"ğŸ’¡ Make sure you have set up your OpenAI API key in a .env file\")\n",
    "    print(\"ğŸ“ For now, let's continue with the concepts!\")\n",
    "    \n",
    "    # Simulated response for demonstration\n",
    "    print(\"\\nğŸ¤– Simulated LLM Response:\")\n",
    "    print(\"Why do programmers prefer dark mode? Because light attracts bugs! ğŸ›\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "### ğŸ¯ Understanding the Code\n",
    "\n",
    "Let's break down what we just did:\n",
    "\n",
    "1. **`OpenAI(temperature=0.7)`**: Creates a connection to OpenAI's language model\n",
    "   - `temperature=0.7`: Controls randomness (0.0 = very predictable, 1.0 = very creative)\n",
    "\n",
    "2. **`llm.predict()`**: Sends a prompt to the LLM and gets a response\n",
    "\n",
    "### ğŸ”§ Key Parameters\n",
    "\n",
    "- **Temperature**: Controls creativity\n",
    "- **Max Tokens**: Limits response length\n",
    "- **Model**: Which LLM to use (e.g., 'gpt-3.5-turbo', 'gpt-4')\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ¯ Quick Exercise\n",
    "\n",
    "**Try this**: What would happen if you changed the temperature to 0.0? What about 1.0?\n",
    "\n",
    "Write your prediction below:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "**Your Prediction:**\n",
    "- Temperature 0.0: [Your guess]\n",
    "- Temperature 1.0: [Your guess]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## ğŸ¯ Section 2: Working with Prompts\n",
    "\n",
    "Prompts are like instructions for the LLM. Let's learn how to create effective prompts!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“ Simple Prompt:\n",
      "Explain what machine learning is in one sentence.\n"
     ]
    }
   ],
   "source": [
    "# Method 1: Simple string prompt\n",
    "simple_prompt = \"Explain what machine learning is in one sentence.\"\n",
    "\n",
    "print(\"ğŸ“ Simple Prompt:\")\n",
    "print(simple_prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“ Simple Prompt:\n",
      "What is the capital of France?\n",
      "--------------------------------------------------\n",
      "ğŸ¤– LLM Response:\n",
      "The capital of France is Paris.\n"
     ]
    }
   ],
   "source": [
    "# Your simple prompt here:\n",
    "simple_prompt = \"What is the capital of France?\"\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model_name=\"gpt-4.1-nano-2025-04-14\",  \n",
    "    temperature=0,                       \n",
    "    max_tokens=256)  \n",
    "    \n",
    "response = llm.predict(simple_prompt)\n",
    "\n",
    "print(\"ğŸ“ Simple Prompt:\")\n",
    "print(simple_prompt)\n",
    "print(\"-\"*50)\n",
    "print(\"ğŸ¤– LLM Response:\")\n",
    "print(response)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“ Prompt Template:\n",
      "Explain {topic} to a {audience} in {length} sentences.\n",
      "\n",
      "ğŸ“‹ Variables: ['audience', 'length', 'topic']\n",
      "\n",
      "ğŸ¯ Formatted Prompt:\n",
      "Explain artificial intelligence to a rocket scientist in 3 sentences.\n",
      "--------------------------------------------------\n",
      "ğŸ¤– LLM Response:\n",
      "Artificial intelligence (AI) refers to the development of algorithms and systems that enable machines to perform tasks typically requiring human cognition, such as learning, reasoning, and problem-solving. In the context of rocket science, AI can optimize trajectory planning, autonomous navigation, and system diagnostics, enhancing mission efficiency and safety. Essentially, AI acts as an intelligent assistant that processes vast data streams and adapts to complex, dynamic environments, much like advanced control systems but with greater flexibility and learning capability.\n"
     ]
    }
   ],
   "source": [
    "# Method 2: PromptTemplate (more powerful)\n",
    "template = PromptTemplate(\n",
    "    input_variables=[\"topic\", \"audience\", \"length\"],\n",
    "    template=\"Explain {topic} to a {audience} in {length} sentences.\"\n",
    ")\n",
    "\n",
    "print(\"ğŸ“ Prompt Template:\")\n",
    "print(template.template)\n",
    "print(\"\\nğŸ“‹ Variables:\", template.input_variables)\n",
    "\n",
    "# Example usage\n",
    "formatted_prompt = template.format(\n",
    "    topic=\"artificial intelligence\",\n",
    "    audience=\"rocket scientist\",\n",
    "    length=\"3\"\n",
    ")\n",
    "\n",
    "print(\"\\nğŸ¯ Formatted Prompt:\")\n",
    "print(formatted_prompt)\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model_name=\"gpt-4.1-nano-2025-04-14\",  \n",
    "    temperature=0,                       \n",
    "    max_tokens=256)  \n",
    "    \n",
    "response = llm.predict(formatted_prompt)\n",
    "\n",
    "print(\"-\"*50)\n",
    "print(\"ğŸ¤– LLM Response:\")\n",
    "print(response)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ›« Prompt Template:\n",
      "×”×¦×¢ ×™×¢×“ ×˜×™×¡×” ××•×©×œ× ×œ×¢×•× ×” ×©×œ {season}, ×‘×ª×§×¦×™×‘ ×©×œ {budget}, ×¢×‘×•×¨ ××™×©×”×• ×©××•×”×‘ {interests}, ×•××ª×›× ×Ÿ ×—×•×¤×©×” ×©×œ {duration} ×™××™×. ×”×¡×‘×¨ ×‘×§×¦×¨×” ×œ××” ×‘×—×¨×ª ×“×•×•×§× ××ª ×”×™×¢×“ ×”×–×”.\n",
      "\n",
      "ğŸ“‹ Variables: ['budget', 'duration', 'interests', 'season']\n",
      "--------------------------------------------------\n",
      "ğŸ¤– LLM Response:\n",
      "×”×™×¢×“ ×”××•×©×œ× ×¢×‘×•×¨×š ×”×•× ×œ×™×¡×‘×•×Ÿ, ×¤×•×¨×˜×•×’×œ. ×‘×¢×•× ×” ×©×œ ××‘×™×‘, ×œ×™×¡×‘×•×Ÿ ××¦×™×¢×” × ×•×¤×™× ××¨×”×™×‘×™×\n",
      "×©×œ ×’×‘×¢×•×ª, × ×”×¨×•×ª ×•×”×¢×™×¨ ×”×¢×ª×™×§×”, ×¢× ×¤×¨×—×™× ×¤×•×¨×—×™× ×‘××•×•×™×¨. ×ª×§×¦×™×‘ ×©×œ 3000 ×©×´×— ×××¤×©×¨\n",
      "×˜×™×¡×•×ª ×”×œ×•×š ×•×©×•×‘ ×•××’×•×¨×™× ×¡×‘×™×¨×™×. ×‘×¢×™×¨ ×ª×•×›×œ ×œ×™×”× ×•×ª ×××•×›×œ ××§×•××™ ×˜×¢×™× ×›××• ×¤×¡×˜×œ ×“×”\n",
      "× ××¤×”, ×•×œ×—×§×•×¨ ××ª ×”×¢×™×¨ ×‘×¨×’×œ×™×™× ×‘×¨×—×•×‘×•×ª ×”×¦×™×•×¨×™×™×, ×‘×˜×™×™×œ×•×ª ×¢×œ × ×”×¨ ×”×˜×’×•×¡ ×•×‘××ª×¨×™×\n",
      "×”×”×™×¡×˜×•×¨×™×™×. ×œ×™×¡×‘×•×Ÿ ××©×œ×‘×ª × ×•×¤×™× ××¨×”×™×‘×™×, ×ª×¨×‘×•×ª ×§×•×œ×™× ×¨×™×ª ×¢×©×™×¨×” ×•××¤×©×¨×•×™×•×ª ×œ×˜×™×•×œ×™×\n",
      "×¨×’×œ×™×™×, ×•×”×™× ××ª××™××” ×œ×—×•×¤×©×” ×©×œ 5 ×™××™× ××œ××” ×‘×—×•×•×™×•×ª.\n"
     ]
    }
   ],
   "source": [
    "template = PromptTemplate(\n",
    "    input_variables=[\"season\", \"budget\", \"interests\", \"duration\"],\n",
    "    template=\"×”×¦×¢ ×™×¢×“ ×˜×™×¡×” ××•×©×œ× ×œ×¢×•× ×” ×©×œ {season}, ×‘×ª×§×¦×™×‘ ×©×œ {budget}, ×¢×‘×•×¨ ××™×©×”×• ×©××•×”×‘ {interests}, ×•××ª×›× ×Ÿ ×—×•×¤×©×” ×©×œ {duration} ×™××™×. ×”×¡×‘×¨ ×‘×§×¦×¨×” ×œ××” ×‘×—×¨×ª ×“×•×•×§× ××ª ×”×™×¢×“ ×”×–×”.\"\n",
    ")\n",
    "\n",
    "print(\"ğŸ›« Prompt Template:\")\n",
    "print(template.template)\n",
    "print(\"\\nğŸ“‹ Variables:\", template.input_variables)\n",
    "\n",
    "\n",
    "formatted_prompt = template.format(\n",
    "    season=\"××‘×™×‘\",\n",
    "    budget=\"3000 ×©×´×—\",\n",
    "    interests=\"× ×•×¤×™×, ××•×›×œ ××§×•××™ ×•×˜×™×•×œ×™× ×¨×’×œ×™×™×\",\n",
    "    duration=\"5\"\n",
    ")\n",
    "\n",
    "\n",
    "response = llm.predict(formatted_prompt)\n",
    "\n",
    "print(\"-\"*50)\n",
    "print(\"ğŸ¤– LLM Response:\")\n",
    "# Print response with proper line wrapping for better readability\n",
    "import textwrap\n",
    "wrapped_response = textwrap.fill(response, width=80)\n",
    "print(wrapped_response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your prompt template here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¤– Generated Story Prompt:\n",
      "\n",
      "    Write a short story with the following requirements:\n",
      "    - Main character: a robot who loves cooking\n",
      "    - Setting: a futuristic kitchen on Mars\n",
      "    - Genre: science fiction comedy\n",
      "    - Length: 2-3 paragraphs\n",
      "    - Make it engaging and creative\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "# Let's create a more complex prompt template\n",
    "story_template = PromptTemplate(\n",
    "    input_variables=[\"character\", \"setting\", \"genre\"],\n",
    "    template=\"\"\"\n",
    "    Write a short story with the following requirements:\n",
    "    - Main character: {character}\n",
    "    - Setting: {setting}\n",
    "    - Genre: {genre}\n",
    "    - Length: 2-3 paragraphs\n",
    "    - Make it engaging and creative\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "# Create a story prompt\n",
    "story_prompt = story_template.format(\n",
    "    character=\"a robot who loves cooking\",\n",
    "    setting=\"a futuristic kitchen on Mars\",\n",
    "    genre=\"science fiction comedy\"\n",
    ")\n",
    "\n",
    "print(\"ğŸ¤– Generated Story Prompt:\")\n",
    "print(story_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¤– Generated Story Prompt:\n",
      "\n",
      "    Write a short recipe with the following requirements:\n",
      "    - Main ingridiants: cheese, tomato\n",
      "    - Allergens: gluten free\n",
      "    - Style: italian\n",
      "    - Simplicity: the recipe should be simple for a mom to make with a 10 year old child\n",
      "    - Make it engaging and creative\n",
      "    \n",
      "    write the recipe as JSON format\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "# Let's create a more complex prompt template\n",
    "story_template = PromptTemplate(\n",
    "    input_variables=[\"ingridiants\", \"allergens\", \"style\"],\n",
    "    template=\"\"\"\n",
    "    Write a short recipe with the following requirements:\n",
    "    - Main ingridiants: {ingridiants}\n",
    "    - Allergens: {allergens}\n",
    "    - Style: {style}\n",
    "    - Simplicity: the recipe should be simple for a mom to make with a 10 year old child\n",
    "    - Make it engaging and creative\n",
    "    \n",
    "    write the recipe as JSON format\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "# Create a story prompt\n",
    "story_prompt = story_template.format(\n",
    "    ingridiants=\"cheese, tomato\",\n",
    "    allergens= 'gluten free',\n",
    "    style= 'italian'\n",
    ")\n",
    "\n",
    "print(\"ğŸ¤– Generated Story Prompt:\")\n",
    "print(story_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¤– LLM Response:\n",
      "Based on the provided details, the estimated average price for a house in Tel Aviv measuring approximately 100 square meters with three rooms and located on or below the third floor typically ranges between â‚ª3.5 million to â‚ª4.5 million. Tel Aviv is known for its high real estate prices due to its status as a major economic and cultural hub, limited land availability, and high demand for residential properties. The size of 100 square meters is considered a comfortable, medium-sized apartment, which is highly sought after in the city.\n",
      "\n",
      "The location within Tel Aviv significantly influences the price. Properties closer to the city center, beaches, or popular neighborhoods tend to command higher prices. Since the house is not higher than the third floor, it may be slightly less expensive than units on higher floors with better views, but this can also be a desirable feature for those seeking easier access and lower noise levels. Overall, the price reflects the premium market conditions in Tel Aviv, the size and layout of the apartment, and its relatively accessible floor level.\n",
      "ğŸ¤– Generated Story Prompt:\n"
     ]
    }
   ],
   "source": [
    "story_template = PromptTemplate(\n",
    "    input_variables=[\"house_in_meter\", \"number_of_rooms\", \"location\", \"other\"],\n",
    "    template=\"\"\"\n",
    "    give assessment price or average price of the following house:\n",
    "    - house in meter: {house_in_meter}\n",
    "    - number of rooms: {number_of_rooms}\n",
    "    - location: {location}\n",
    "    - Other (free text by the user): {other}\n",
    "    - Length: 2-3 paragraphs\n",
    "    - give me Explanation why this is the price\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "# Create a story prompt\n",
    "story_prompt = story_template.format(\n",
    "    house_in_meter=100,\n",
    "    number_of_rooms=3,\n",
    "    location=\"Tel Aviv\",\n",
    "    other=\"the house is not higher then  3rd floor\"\n",
    ")\n",
    "response = llm.predict(story_prompt)\n",
    "print(\"ğŸ¤– LLM Response:\")\n",
    "print(response)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "ğŸ¤– LLM Response:\n",
      "{   \"recipe_name\": \"Mini Caprese Pizza Bites\",   \"style\": \"Italian\",\n",
      "\"description\": \"A fun and easy Italian-inspired snack perfect for a mom and\n",
      "child to make together! These cheesy tomato bites are gluten-free and bursting\n",
      "with flavor.\",   \"ingredients\": [     {       \"name\": \"Fresh mozzarella cheese\",\n",
      "\"quantity\": \"1 cup, cubed\"     },     {       \"name\": \"Cherry tomatoes\",\n",
      "\"quantity\": \"1 cup, halved\"     },     {       \"name\": \"Gluten-free crackers or\n",
      "rice cakes\",       \"quantity\": \"4-6 pieces\"     },     {       \"name\": \"Fresh\n",
      "basil leaves (optional)\",       \"quantity\": \"A few for garnish\"     },     {\n",
      "\"name\": \"Olive oil\",       \"quantity\": \"1 teaspoon\"     },     {       \"name\":\n",
      "\"Salt and pepper\",       \"quantity\": \"To taste\"     }   ],   \"instructions\": [\n",
      "{       \"step\": 1,       \"description\": \"Lay out the gluten-free crackers or\n",
      "rice cakes on a plate.\"     },     {       \"step\": 2,       \"description\": \"Top\n",
      "each cracker with a\n"
     ]
    }
   ],
   "source": [
    "response = llm.predict(story_prompt) #respons = the parameter that saves the response, llm = the model that is used to generate the response, predict = the method that is used to generate the response (prompt)\n",
    "\n",
    "print(\"-\"*50)\n",
    "print(\"ğŸ¤– LLM Response:\")\n",
    "# Print response with proper line wrapping for better readability\n",
    "import textwrap\n",
    "wrapped_response = textwrap.fill(response, width=80)\n",
    "print(wrapped_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Mini Caprese Pizza Bites** \n",
    " \n",
    "**Ingredients:** - Fresh mozzarella cheese (sliced\n",
    "into small rounds) - Cherry tomatoes (halved) - Fresh basil leaves (optional) -\n",
    "Olive oil - Balsamic glaze (optional) - Gluten-free crackers or rice cakes\n",
    "**Instructions:**  \n",
    "1. **Get Creative:** Lay out your gluten-free crackers or\n",
    "rice cakes on a plateâ€”these are your pizza bases!  \n",
    "2. **Add the Cheese:** Place\n",
    "a small slice of mozzarella on each cracker or rice cake. Itâ€™s like making tiny\n",
    "cheese pizzas!  \n",
    "3. **Top with Tomatoes:** Add a halved cherry tomato on top of\n",
    "the cheese. The juicy tomato adds a burst of flavor!  \n",
    "4. **Garnish:** If you\n",
    "have basil, tuck a small leaf on top for that authentic Italian touch.  \n",
    "5.**Finish with Flair:** Drizzle a little olive oil over the bites, and if you\n",
    "like, a tiny drizzle of balsamic glaze for sweetness.  **Enjoy your homemade\n",
    "Italian-inspired cheese and tomato bitesâ€”perfect for a quick snack or a fun\n",
    "appetizer!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your story prompt here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“– Generated Story:\n",
      "**Mini Caprese Pizza Bites**\n",
      "\n",
      "**Ingredients:**\n",
      "- Fresh mozzarella cheese (sliced into small rounds)\n",
      "- Cherry tomatoes (halved)\n",
      "- Fresh basil leaves (optional)\n",
      "- Olive oil\n",
      "- Balsamic glaze (optional)\n",
      "- Gluten-free crackers or rice cakes\n",
      "\n",
      "**Instructions:**\n",
      "\n",
      "1. **Get Creative:** Lay out your gluten-free crackers or rice cakes on a plateâ€”these are your pizza bases!\n",
      "\n",
      "2. **Build the Bites:** Place a slice of mozzarella on each cracker. Top with a halved cherry tomato and a small basil leaf if you have one.\n",
      "\n",
      "3. **Add a Drizzle:** Lightly drizzle olive oil over each bite for that authentic Italian flavor. If you like, add a tiny splash of balsamic glaze for a sweet tang.\n",
      "\n",
      "4. **Enjoy:** These mini \"pizzas\" are perfect for a quick snack or appetizer. Let your little chef help with placing the toppingsâ€”it's fun and easy!\n",
      "\n",
      "**Tip:** For extra flavor, sprinkle a pinch of salt or Italian herbs if you have them.\n",
      "\n",
      "**Buon appetito!**\n"
     ]
    }
   ],
   "source": [
    "# Try it with the LLM\n",
    "try:\n",
    "    response = llm.predict(story_prompt)\n",
    "    print(\"\\nğŸ“– Generated Story:\")\n",
    "    print(response)\n",
    "except:\n",
    "    print(\"\\nğŸ“– Simulated Story Response:\")\n",
    "    print(\"\"\"\n",
    "    In the year 2157, ChefBot-3000 whirred around the Martian kitchen, its metallic arms\n",
    "    expertly chopping space vegetables. The red planet's gravity made cooking an adventure,\n",
    "    but nothing could dampen this robot's passion for creating the perfect Mars stew.\n",
    "    \n",
    "    When the human colonists complained about the floating ingredients, ChefBot simply\n",
    "    replied, \"That's not a bug, it's a feature!\" and continued its culinary masterpiece.\n",
    "    \"\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "### ğŸ¯ Prompt Engineering Tips\n",
    "\n",
    "1. **Be Specific**: Instead of \"Write about AI\", say \"Write a 3-sentence explanation of AI for beginners\"\n",
    "2. **Use Variables**: Make prompts reusable with templates\n",
    "3. **Set Constraints**: Specify length, tone, format\n",
    "4. **Provide Context**: Give the LLM enough information to work with\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ¯ Quick Exercise\n",
    "\n",
    "**Create your own prompt template!**\n",
    "\n",
    "Design a prompt template for explaining complex topics. What variables would you include?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "**Your Prompt Template:**\n",
    "\n",
    "```\n",
    "Template: [Write your template here]\n",
    "Variables: [List your variables]\n",
    "```\n",
    "\n",
    "**Example:**\n",
    "```\n",
    "Template: Explain {topic} to a {audience} using {style} language in {length} sentences.\n",
    "Variables: ['topic', 'audience', 'style', 'length']\n",
    "```\n",
    "\n",
    "Step 1: Create a prompt template\n",
    " \n",
    " ```python\n",
    " explain_template = PromptTemplate(\n",
    "     input_variables=[\"concept\"],\n",
    "     template=\"Explain {concept} in simple terms that a beginner can understand.\")\n",
    " ```\n",
    "\n",
    " Step 2: Create a chain\n",
    "```python\n",
    "explain_chain = LLMChain(llm=llm, prompt=explain_template)\n",
    "\n",
    "print(\"ğŸ”— Chain Created Successfully!\")\n",
    "print(\"Chain components:\")\n",
    "print(f\"- LLM: {type(llm).__name__}\")\n",
    "print(f\"- Prompt Template: {explain_template.template}\")\n",
    "print(f\"- Input Variables: {explain_template.input_variables}\")\n",
    " ```\n",
    "OUTPUT:\n",
    "\n",
    "ğŸ”— Chain Created Successfully!\n",
    "Chain components:\n",
    "- LLM: OpenAI\n",
    "- Prompt Template: Explain {concept} in simple terms that a beginner can understand.\n",
    "- Input Variables: ['concept']\n",
    "\n",
    "\n",
    "Step 3: Use the chain\n",
    "```python\n",
    "concepts = [\"blockchain\", \"machine learning\", \"cloud computing\"]\n",
    "\n",
    "print(\"ğŸ”— Running the Chain:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for concept in concepts:\n",
    "    try:\n",
    "        response = explain_chain.invoke(concept)\n",
    "        print(f\"\\nğŸ“š {concept.upper()}:\")\n",
    "        print(response)\n",
    "        print(\"-\"*40)\n",
    "    except:\n",
    "        print(f\"\\nğŸ“š {concept.upper()}:\")\n",
    "        print(f\"[Simulated explanation of {concept} for beginners]\")\n",
    "        print(\"-\"*40)\n",
    "```\n",
    "\n",
    "OUTPUT:\n",
    "\n",
    "Running the Chain:\n",
    "\n",
    "BLOCKCHAIN:\n",
    "\n",
    "Blockchain is like a digital ledger or record book where information is stored in blocks that are linked together in a chain. Each block contains data and a unique code called a hash. Once a block is created, it is added to the chain and cannot be changed. This makes it very secure and reliable. The information on the blockchain is stored on many different computers, so it is decentralized and cannot be controlled by one person or entity. This technology is commonly used for recording and tracking transactions, like buying and selling cryptocurrencies, but it can also be used for other types of data storage and sharing.\n",
    "\n",
    "MACHINE LEARNING:\n",
    "\n",
    "Machine learning is a type of technology that allows computers to learn and make decisions without being explicitly programmed to do so. It involves feeding large amounts of data into a computer and using algorithms to analyze and find patterns within the data. The computer then uses these patterns to make predictions or decisions about new data it receives. It is like teaching a child how to ride a bike - at first, you show them how to do it, but eventually, they learn on their own and can ride without your help. Similarly, machine learning algorithms learn from the data they are given and improve over time, making them useful for tasks like predicting stock prices, recognizing faces in photos, and even diagnosing diseases.\n",
    "\n",
    "\n",
    "CLOUD COMPUTING:\n",
    "\n",
    "Cloud computing refers to the delivery of computing services over the internet. This means that instead of storing data or running programs on your own computer, you can access them from a remote server through the internet. This allows you to use and store data on the cloud without needing to have physical hardware or infrastructure. It is like renting storage space or using software on someone else's computer, but you can access it from anywhere as long as you have an internet connection. This makes it easier and more convenient to use technology without having to worry about maintenance, updates, or storage space on your own device.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## ğŸ¯ Section 3: Building Your First Chain\n",
    "\n",
    "Chains are like assembly lines for AI operations. Let's build one!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "llm = ChatOpenAI(\n",
    "    model_name=\"gpt-4.1-nano-2025-04-14\",  \n",
    "    temperature=0,                       \n",
    "    max_tokens=256)  \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Create a prompt template\n",
    "explain_template = PromptTemplate(\n",
    "    input_variables=[\"ADD HERE\"],\n",
    "    template=\"ADD YOUR TEMPLATE HERE, USE {} FOR THE INPUT VARIABLES.\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 2: PromptTemplate (more powerful)\n",
    "coding_template  = PromptTemplate(\n",
    "    input_variables=[\"programming_language\"],\n",
    "    template=\"Write 'hello world' using {programming_language}.\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”— Chain Created Successfully!\n",
      "Chain components:\n",
      "- LLM: ChatOpenAI\n",
      "- Prompt Template: Write 'hello world' using {programming_language}.\n",
      "- Input Variables: ['programming_language']\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Create a chain\n",
    "explain_chain = LLMChain(llm=llm, prompt=coding_template)\n",
    "\n",
    "print(\"ğŸ”— Chain Created Successfully!\")\n",
    "print(\"Chain components:\")\n",
    "print(f\"- LLM: {type(llm).__name__}\")\n",
    "print(f\"- Prompt Template: {explain_template.template}\")\n",
    "print(f\"- Input Variables: {explain_template.input_variables}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”— Running the Chain:\n",
      "============================================================\n",
      "\n",
      "ğŸ“š PYTHON:\n",
      "{'programming_language': 'python', 'text': 'Sure! Here\\'s a simple Python program that prints \"hello world\" to the console:\\n\\n```python\\nprint(\"hello world\")\\n```'}\n",
      "----------------------------------------\n",
      "\n",
      "ğŸ“š JAVA:\n",
      "{'programming_language': 'java', 'text': 'Certainly! Here\\'s a simple Java program that prints \"hello world\" to the console:\\n\\n```java\\npublic class HelloWorld {\\n    public static void main(String[] args) {\\n        System.out.println(\"hello world\");\\n    }\\n}\\n```\\n\\nTo run this program:\\n1. Save it in a file named `HelloWorld.java`.\\n2. Open your terminal or command prompt.\\n3. Compile the program with: `javac HelloWorld.java`\\n4. Run the compiled program with: `java HelloWorld`\\n\\nYou should see the output:\\n```\\nhello world\\n```'}\n",
      "----------------------------------------\n",
      "\n",
      "ğŸ“š JAVASCRIPT:\n",
      "{'programming_language': 'javascript', 'text': 'Sure! Here\\'s a simple example of how to write \"hello world\" using JavaScript:\\n\\n```javascript\\nconsole.log(\"hello world\");\\n```\\n\\nThis code will print \"hello world\" to the browser\\'s console. If you\\'d like to display it on a webpage, you can use:\\n\\n```html\\n<!DOCTYPE html>\\n<html>\\n<head>\\n    <title>Hello World</title>\\n</head>\\n<body>\\n    <script>\\n        document.write(\"hello world\");\\n    </script>\\n</body>\\n</html>\\n```\\n\\nLet me know if you\\'d like a different example!'}\n",
      "----------------------------------------\n",
      "\n",
      "ğŸ“š ASSEMBLY:\n",
      "{'programming_language': 'assembly', 'text': 'Certainly! Here\\'s a simple \"Hello, World!\" program written in x86 assembly for Linux using NASM syntax:\\n\\n```assembly\\nsection .data\\n    msg     db      \\'Hello, World!\\', 0xA  ; Message with newline\\n    len     equ     $ - msg               ; Length of the message\\n\\nsection .text\\n    global  _start\\n\\n_start:\\n    ; write the message to stdout\\n    mov     eax, 4          ; sys_write system call\\n    mov     ebx, 1          ; file descriptor 1 = stdout\\n    mov     ecx, msg        ; pointer to message\\n    mov     edx, len        ; message length\\n    int     0x80            ; call kernel\\n\\n    ; exit the program\\n    mov     eax, 1          ; sys_exit system call\\n    xor     ebx, ebx        ; exit code 0\\n    int     0x80            ; call kernel\\n```\\n\\n### How to assemble and run:\\n\\n1. Save the code to a file, e.g., `hello.asm`.\\n2. Assemble with NASM:\\n   ```\\n   nasm -f elf32 hello.asm -o hello.o\\n   ```\\n3. Link with ld:\\n'}\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Use the chain\n",
    "concepts = ['python', 'java', 'javascript', 'assembly']\n",
    "\n",
    "print(\"ğŸ”— Running the Chain:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for concept in concepts:\n",
    "    try:\n",
    "        response = explain_chain.invoke(concept)\n",
    "        print(f\"\\nğŸ“š {concept.upper()}:\")\n",
    "        print(response)\n",
    "        print(\"-\"*40)\n",
    "    except:\n",
    "        print(f\"\\nğŸ“š {concept.upper()}:\")\n",
    "        print(f\"[Simulated explanation of {concept} for beginners]\")\n",
    "        print(\"-\"*40)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¯ Question Generator Chain:\n",
      "==================================================\n",
      "{'topic': 'Python programming', 'difficulty': 'beginner', 'text': 'Q: What is the purpose of the print() function in Python?  \\nQ: How do you create a variable to store the number 10 in Python?  \\nQ: What symbol is used to start a comment in Python code?'}\n"
     ]
    }
   ],
   "source": [
    "# Let's create a more complex chain - a question generator\n",
    "question_template = PromptTemplate(\n",
    "    input_variables=[\"topic\", \"difficulty\"],\n",
    "    template=\"\"\"\n",
    "    Generate 3 {difficulty} questions about {topic}.\n",
    "    Format each question on a new line starting with 'Q: '\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "question_chain = LLMChain(llm=llm, prompt=question_template)\n",
    "\n",
    "print(\"ğŸ¯ Question Generator Chain:\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "try:\n",
    "    questions = question_chain.invoke({\"topic\": \"Python programming\", \"difficulty\": \"beginner\"})\n",
    "    print(questions)\n",
    "except:\n",
    "    print(\"Q: What is a variable in Python?\") \n",
    "    print(\"Q: How do you print 'Hello World' in Python?\") \n",
    "    print(\"Q: What is the difference between a list and a tuple?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR COMPLEX CHAIN HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "### ğŸ”— Understanding Chains\n",
    "\n",
    "**Chains** connect multiple operations together:\n",
    "\n",
    "```\n",
    "Input â†’ Prompt Template â†’ LLM â†’ Output\n",
    "```\n",
    "\n",
    "**Benefits of Chains:**\n",
    "- Reusable components\n",
    "- Consistent formatting\n",
    "- Easy to modify and extend\n",
    "- Can be combined with other chains\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ¯ Quick Exercise\n",
    "\n",
    "**Design a Chain**: What kind of chain would you create for a study assistant? What would the input and output be?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "**Your Study Assistant Chain Design:**\n",
    "\n",
    "**Input:** [What would the user provide?]\n",
    "**Output:** [What would the chain produce?]\n",
    "**Template:** [What would the prompt template look like?]\n",
    "\n",
    "**Example:**\n",
    "- **Input:** Topic (e.g., \"photosynthesis\")\n",
    "- **Output:** Study guide with key points\n",
    "- **Template:** \"Create a study guide for {topic} with main concepts and examples\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## ğŸ¯ Section 4: Memory and Context\n",
    "\n",
    "Memory allows your AI to remember previous conversations. This is crucial for chatbots!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ§  Memory System Created!\n",
      "Memory type: ConversationBufferMemory\n",
      "Current memory: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/4k/bd53pxq144v00900ycf02bch0000gp/T/ipykernel_47143/722529913.py:2: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationBufferMemory()\n"
     ]
    }
   ],
   "source": [
    "# Create a conversation memory\n",
    "memory = ConversationBufferMemory()\n",
    "\n",
    "print(\"ğŸ§  Memory System Created!\")\n",
    "print(f\"Memory type: {type(memory).__name__}\")\n",
    "print(f\"Current memory: {memory.buffer}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ’¬ Conversation Chain with Memory Created!\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Create a conversation chain with memory\n",
    "conversation_template = PromptTemplate(\n",
    "    input_variables=[\"history\", \"human_input\"],\n",
    "    template=\"\"\"\n",
    "    You are a helpful AI assistant. Use the conversation history to provide context-aware responses.\n",
    "    \n",
    "    Conversation History:\n",
    "    {history}\n",
    "    \n",
    "    Human: {human_input}\n",
    "    AI: \"\"\"\n",
    ")\n",
    "\n",
    "conversation_chain = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=conversation_template,\n",
    "    memory=memory,\n",
    "    verbose=True  # Shows the chain's thinking process\n",
    ")\n",
    "\n",
    "print(\"ğŸ’¬ Conversation Chain with Memory Created!\")\n",
    "print(\"=\"*60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ’¬ Simulated Conversation:\n",
      "============================================================\n",
      "\n",
      "ğŸ‘¤ Human 1: Hi! My name is Alex.\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "    You are a helpful AI assistant. Use the conversation history to provide context-aware responses.\n",
      "    \n",
      "    Conversation History:\n",
      "    \n",
      "    \n",
      "    Human: Hi! My name is Alex.\n",
      "    AI: \u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "ğŸ¤– AI 1:  Hi Alex! It's nice to meet you. How can I assist you?\n",
      "----------------------------------------\n",
      "\n",
      "ğŸ‘¤ Human 2: I'm learning Python programming.\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "    You are a helpful AI assistant. Use the conversation history to provide context-aware responses.\n",
      "    \n",
      "    Conversation History:\n",
      "    Human: Hi! My name is Alex.\n",
      "AI:  Hi Alex! It's nice to meet you. How can I assist you?\n",
      "    \n",
      "    Human: I'm learning Python programming.\n",
      "    AI: \u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "ğŸ¤– AI 2: That's great! Is there anything specific you need help with? I can provide resources or answer any questions you may have.\n",
      "----------------------------------------\n",
      "\n",
      "ğŸ‘¤ Human 3: What should I study first?\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "    You are a helpful AI assistant. Use the conversation history to provide context-aware responses.\n",
      "    \n",
      "    Conversation History:\n",
      "    Human: Hi! My name is Alex.\n",
      "AI:  Hi Alex! It's nice to meet you. How can I assist you?\n",
      "Human: I'm learning Python programming.\n",
      "AI: That's great! Is there anything specific you need help with? I can provide resources or answer any questions you may have.\n",
      "    \n",
      "    Human: What should I study first?\n",
      "    AI: \u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "ğŸ¤– AI 3: It's important to start with the basics of Python, such as data types, variables, and loops. From there, you can move on to more advanced topics like functions and object-oriented programming. I can recommend some online courses or tutorials to help you get started. \n",
      "----------------------------------------\n",
      "\n",
      "ğŸ‘¤ Human 4: Can you remind me what my name is?\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "    You are a helpful AI assistant. Use the conversation history to provide context-aware responses.\n",
      "    \n",
      "    Conversation History:\n",
      "    Human: Hi! My name is Alex.\n",
      "AI:  Hi Alex! It's nice to meet you. How can I assist you?\n",
      "Human: I'm learning Python programming.\n",
      "AI: That's great! Is there anything specific you need help with? I can provide resources or answer any questions you may have.\n",
      "Human: What should I study first?\n",
      "AI: It's important to start with the basics of Python, such as data types, variables, and loops. From there, you can move on to more advanced topics like functions and object-oriented programming. I can recommend some online courses or tutorials to help you get started. \n",
      "    \n",
      "    Human: Can you remind me what my name is?\n",
      "    AI: \u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "ğŸ¤– AI 4: Of course, your name is Alex. Is there anything else I can assist you with?\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Simulate a conversation\n",
    "conversation_messages = [\n",
    "    \"Hi! My name is Alex.\",\n",
    "    \"I'm learning Python programming.\",\n",
    "    \"What should I study first?\",\n",
    "    \"Can you remind me what my name is?\"\n",
    "]\n",
    "\n",
    "print(\"ğŸ’¬ Simulated Conversation:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for i, message in enumerate(conversation_messages, 1):\n",
    "    print(f\"\\nğŸ‘¤ Human {i}: {message}\")\n",
    "    \n",
    "    try:\n",
    "        response = conversation_chain.run(message)\n",
    "        print(f\"ğŸ¤– AI {i}: {response}\")\n",
    "    except:\n",
    "        # Simulated responses\n",
    "        if i == 1:\n",
    "            print(\"ğŸ¤– AI 1: Hello Alex! Nice to meet you! How can I help you today?\")\n",
    "        elif i == 2:\n",
    "            print(\"ğŸ¤– AI 2: That's great, Alex! Python is an excellent choice for beginners.\")\n",
    "        elif i == 3:\n",
    "            print(\"ğŸ¤– AI 3: For Python beginners, I'd recommend starting with variables, data types, and basic syntax.\")\n",
    "        elif i == 4:\n",
    "            print(\"ğŸ¤– AI 4: Your name is Alex! You mentioned it at the beginning of our conversation.\")\n",
    "    \n",
    "    print(\"-\"*40)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ§  Current Memory Contents:\n",
      "========================================\n",
      "Human: Hi! My name is Alex.\n",
      "AI:  Hi Alex! It's nice to meet you. How can I assist you?\n",
      "Human: I'm learning Python programming.\n",
      "AI: That's great! Is there anything specific you need help with? I can provide resources or answer any questions you may have.\n",
      "Human: What should I study first?\n",
      "AI: It's important to start with the basics of Python, such as data types, variables, and loops. From there, you can move on to more advanced topics like functions and object-oriented programming. I can recommend some online courses or tutorials to help you get started. \n",
      "Human: Can you remind me what my name is?\n",
      "AI: Of course, your name is Alex. Is there anything else I can assist you with?\n",
      "\n",
      "ğŸ“Š Memory Statistics:\n",
      "- Total messages: 8\n",
      "- Memory type: ConversationBufferMemory\n"
     ]
    }
   ],
   "source": [
    "# Check what's stored in memory\n",
    "print(\"ğŸ§  Current Memory Contents:\")\n",
    "print(\"=\"*40)\n",
    "print(memory.buffer)\n",
    "\n",
    "print(\"\\nğŸ“Š Memory Statistics:\")\n",
    "print(f\"- Total messages: {len(memory.chat_memory.messages)}\")\n",
    "print(f\"- Memory type: {type(memory).__name__}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "### ğŸ§  Types of Memory in LangChain\n",
    "\n",
    "1. **ConversationBufferMemory**: Stores all messages\n",
    "2. **ConversationSummaryMemory**: Summarizes long conversations\n",
    "3. **ConversationTokenBufferMemory**: Limits memory by token count\n",
    "4. **ConversationBufferWindowMemory**: Keeps only recent messages\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ¯ Quick Exercise\n",
    "\n",
    "**Memory Challenge**: If you were building a customer service chatbot, which type of memory would you choose and why?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "**Your Answer:**\n",
    "\n",
    "**Memory Type:** [Your choice]\n",
    "**Reason:** [Why you chose this type]\n",
    "\n",
    "**Hint:** Consider factors like conversation length, privacy, and performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## ğŸ¯ Section 5: Real-World Applications\n",
    "\n",
    "Let's build some practical examples that you might actually use!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” Code Review Assistant Created!\n",
      "==================================================\n",
      "\n",
      "ğŸ“ Code Review:\n",
      "\n",
      "    1. The code takes in a list of numbers and calculates the average of those numbers.\n",
      "    2. - The function could be renamed to better reflect what it does, such as \"calculate_list_average\".\n",
      "       - The function could include error handling for cases where the input is not a list or the list is empty.\n",
      "       - The function could include type checking to ensure that all elements in the list are numbers.\n",
      "    3. - Add comments to explain the purpose of the function and the logic behind it.\n",
      "       - Use descriptive variable names instead of single letters for better readability.\n",
      "       - Consider using the built-in function \"sum()\" instead of a for loop to calculate the total.\n",
      "       - Consider using the built-in function \"statistics.mean()\" instead of manually calculating the average.\n",
      "       - Consider adding a docstring to the function to provide information on its purpose, parameters, and return value.\n"
     ]
    }
   ],
   "source": [
    "# Application 1: Code Review Assistant\n",
    "code_review_template = PromptTemplate(\n",
    "    input_variables=[\"code\", \"language\"],\n",
    "    template=\"\"\"\n",
    "    Review this {language} code and provide feedback:\n",
    "    \n",
    "    Code:\n",
    "    {code}\n",
    "    \n",
    "    Please provide:\n",
    "    1. What the code does\n",
    "    2. Potential improvements\n",
    "    3. Best practices suggestions\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "code_review_chain = LLMChain(llm=llm, prompt=code_review_template)\n",
    "\n",
    "print(\"ğŸ” Code Review Assistant Created!\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Sample code to review\n",
    "sample_code = \"\"\"\n",
    "def calculate_average(numbers):\n",
    "    total = 0\n",
    "    for num in numbers:\n",
    "        total = total + num\n",
    "    return total / len(numbers)\n",
    "\"\"\"\n",
    "\n",
    "try:\n",
    "    review = code_review_chain.run({\"code\": sample_code, \"language\": \"Python\"})\n",
    "    print(\"\\nğŸ“ Code Review:\")\n",
    "    print(review)\n",
    "except:\n",
    "    print(\"\\nğŸ“ Simulated Code Review:\")\n",
    "    print(\"\"\"\n",
    "    1. What the code does: Calculates the average of a list of numbers\n",
    "    \n",
    "    2. Potential improvements:\n",
    "       - Add input validation for empty lists\n",
    "       - Use sum() function instead of manual loop\n",
    "       - Add type hints\n",
    "    \n",
    "    3. Best practices suggestions:\n",
    "       - Handle division by zero\n",
    "       - Add docstring\n",
    "       - Consider using statistics.mean()\n",
    "    \"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“š Study Guide Generator Created!\n",
      "==================================================\n",
      "\n",
      "ğŸ“– Study Guide:\n",
      "\n",
      "1. Key Concepts and Definitions\n",
      "- Function: A set of instructions or statements that perform a specific task.\n",
      "- Parameters: Values that are passed into a function to be used as variables within the function.\n",
      "- Arguments: The actual values that are passed into a function when it is called.\n",
      "- Return: The value that a function outputs after it has completed its task.\n",
      "- Scope: The area of a program where a variable can be accessed.\n",
      "- Local vs Global Variables: Local variables are only accessible within the function they are defined in, while global variables can be accessed from anywhere in the program.\n",
      "- Function Call: The act of using a function in your code by using its name and passing in any necessary arguments.\n",
      "- Recursion: A function calling itself within its own code.\n",
      "\n",
      "2. Important Examples\n",
      "- Basic function syntax:\n",
      "```\n",
      "def function_name(parameters):\n",
      "    # Code block\n",
      "    return value\n",
      "```\n",
      "\n",
      "- Function with no parameters:\n",
      "```\n",
      "def hello():\n",
      "    print(\"Hello!\")\n",
      "```\n",
      "\n",
      "- Function with parameters:\n",
      "```\n",
      "def multiply(x, y):\n",
      "    return x * y\n",
      "```\n",
      "\n",
      "- Function with default parameter values:\n",
      "```\n",
      "def power(base, exponent=2):\n",
      "    return base ** exponent\n",
      "```\n",
      "\n",
      "- Function with keyword arguments:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Application 2: Study Guide Generator\n",
    "study_guide_template = PromptTemplate(\n",
    "    input_variables=[\"topic\", \"level\"],\n",
    "    template=\"\"\"\n",
    "    Create a comprehensive study guide for {topic} at {level} level.\n",
    "    \n",
    "    Include:\n",
    "    1. Key concepts and definitions\n",
    "    2. Important examples\n",
    "    3. Common mistakes to avoid\n",
    "    4. Practice questions\n",
    "    5. Additional resources\n",
    "    \n",
    "    Format it clearly with headers and bullet points.\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "study_guide_chain = LLMChain(llm=llm, prompt=study_guide_template)\n",
    "\n",
    "print(\"ğŸ“š Study Guide Generator Created!\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "try:\n",
    "    guide = study_guide_chain.run({\"topic\": \"functions in programming\", \"level\": \"beginner\"})\n",
    "    print(\"\\nğŸ“– Study Guide:\")\n",
    "    print(guide)\n",
    "except:\n",
    "    print(\"\\nğŸ“– Simulated Study Guide:\")\n",
    "    print(\"\"\"\n",
    "    # Functions in Programming - Beginner Guide\n",
    "    \n",
    "    ## Key Concepts\n",
    "    - Functions are reusable blocks of code\n",
    "    - They take inputs (parameters) and return outputs\n",
    "    - They help organize and modularize code\n",
    "    \n",
    "    ## Important Examples\n",
    "    ```python\n",
    "    def greet(name):\n",
    "        return f\"Hello, {name}!\"\n",
    "    ```\n",
    "    \n",
    "    ## Common Mistakes\n",
    "    - Forgetting to call the function\n",
    "    - Not returning values when needed\n",
    "    - Incorrect parameter order\n",
    "    \n",
    "    ## Practice Questions\n",
    "    1. What is the difference between parameters and arguments?\n",
    "    2. How do you create a function that returns multiple values?\n",
    "    \n",
    "    ## Additional Resources\n",
    "    - Python documentation on functions\n",
    "    - Online tutorials and practice exercises\n",
    "    \"\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## ğŸ¯ Section 6: Advanced Concepts (Preview)\n",
    "\n",
    "Here's a glimpse of what you can do with more advanced LangChain features:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”— Sequential Chain Example:\n",
      "========================================\n",
      "ğŸ“‹ Chain Flow:\n",
      "Interest â†’ Topic â†’ Questions â†’ Answers\n",
      "\n",
      "ğŸ¯ This creates a complete learning module automatically!\n"
     ]
    }
   ],
   "source": [
    "# Sequential Chains - Multiple operations in sequence\n",
    "print(\"ğŸ”— Sequential Chain Example:\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "# Step 1: Generate a topic\n",
    "topic_template = PromptTemplate(\n",
    "    input_variables=[\"interest\"],\n",
    "    template=\"Generate an interesting topic related to {interest}.\"\n",
    ")\n",
    "\n",
    "# Step 2: Create questions about that topic\n",
    "question_template = PromptTemplate(\n",
    "    input_variables=[\"topic\"],\n",
    "    template=\"Create 3 questions about {topic}.\"\n",
    ")\n",
    "\n",
    "# Step 3: Answer those questions\n",
    "answer_template = PromptTemplate(\n",
    "    input_variables=[\"topic\", \"questions\"],\n",
    "    template=\"\"\"\n",
    "    Topic: {topic}\n",
    "    Questions: {questions}\n",
    "    \n",
    "    Provide detailed answers to each question.\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "print(\"ğŸ“‹ Chain Flow:\")\n",
    "print(\"Interest â†’ Topic â†’ Questions â†’ Answers\")\n",
    "print(\"\\nğŸ¯ This creates a complete learning module automatically!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## ğŸ¯ Final Challenge\n",
    "\n",
    "**Design Your Own LangChain Application!**\n",
    "\n",
    "Think about a problem you'd like to solve with AI. Design a LangChain application for it:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "**Your Application Design:**\n",
    "\n",
    "**Application Name:** [Your idea]\n",
    "\n",
    "**Problem it solves:** [What problem does it address?]\n",
    "\n",
    "**Components needed:**\n",
    "- LLM: [Which model?]\n",
    "- Prompts: [What prompts would you use?]\n",
    "- Chains: [How would you structure the flow?]\n",
    "- Memory: [Would it need memory?]\n",
    "- Tools: [Any external integrations?]\n",
    "\n",
    "**Example:**\n",
    "- **Name:** Personal Fitness Coach\n",
    "- **Problem:** Helps users create workout plans\n",
    "- **Components:** LLM for advice, prompts for workout generation, memory for tracking progress, tools for exercise database\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## ğŸ‰ Congratulations!\n",
    "\n",
    "You've completed your introduction to LangChain! Here's what you've learned:\n",
    "\n",
    "### âœ… Key Takeaways\n",
    "\n",
    "1. **LangChain Basics**: Understanding the framework and its components\n",
    "2. **LLMs and Prompts**: How to work with language models effectively\n",
    "3. **Chains**: Building reusable AI workflows\n",
    "4. **Memory**: Maintaining context in conversations\n",
    "5. **Real Applications**: Practical examples you can build upon\n",
    "\n",
    "### ğŸš€ Next Steps\n",
    "\n",
    "1. **Practice**: Try building your own chains and applications\n",
    "2. **Explore**: Check out LangChain's documentation and examples\n",
    "3. **Experiment**: Combine different components to create unique solutions\n",
    "4. **Learn More**: Dive into advanced topics like agents, tools, and embeddings\n",
    "\n",
    "### ğŸ“š Resources\n",
    "\n",
    "- [LangChain Documentation](https://python.langchain.com/)\n",
    "- [LangChain GitHub](https://github.com/langchain-ai/langchain)\n",
    "- [OpenAI API Documentation](https://platform.openai.com/docs)\n",
    "- [Prompt Engineering Guide](https://www.promptingguide.ai/)\n",
    "\n",
    "### ğŸ¯ Remember\n",
    "\n",
    "LangChain is like a toolkit - the more you practice, the more powerful your AI applications become! Start small, experiment often, and don't be afraid to make mistakes. Every great AI application started with a simple chain. ğŸš€\n",
    "\n",
    "**Happy coding!** ğŸ’»âœ¨\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## ğŸ“ Additional Exercises\n",
    "\n",
    "Try these exercises to reinforce your learning:\n",
    "\n",
    "### Exercise 1: Email Generator\n",
    "Create a chain that generates professional emails based on the recipient and purpose.\n",
    "\n",
    "### Exercise 2: Language Translator\n",
    "Build a chain that translates text between different languages.\n",
    "\n",
    "### Exercise 3: Quiz Generator\n",
    "Create a system that generates quizzes on any topic with multiple-choice questions.\n",
    "\n",
    "### Exercise 4: Code Debugger\n",
    "Build a chain that helps debug Python code by analyzing error messages.\n",
    "\n",
    "### Exercise 5: Personal Assistant\n",
    "Design a conversational assistant that can help with daily tasks and remember user preferences.\n",
    "\n",
    "---\n",
    "\n",
    "**Share your creations and discoveries with the community!** ğŸŒŸ\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
