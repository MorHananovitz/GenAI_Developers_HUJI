{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# üöÄ Introduction to LangChain for Beginners\n",
    "\n",
    "Welcome to your first steps into the world of LangChain! This notebook will guide you through the fundamentals of LangChain, a powerful framework for building applications with Large Language Models (LLMs).\n",
    "\n",
    "## üìö What You'll Learn\n",
    "\n",
    "1. **What is LangChain?** - Understanding the basics\n",
    "2. **Core Components** - LLMs, Prompts, Chains, and more\n",
    "3. **Building Your First Chain** - Hands-on examples\n",
    "4. **Working with Prompts** - Creating effective prompts\n",
    "5. **Memory and Context** - Maintaining conversation history\n",
    "6. **Real-world Applications** - Practical examples\n",
    "\n",
    "## üéØ Prerequisites\n",
    "\n",
    "Before starting, make sure you have:\n",
    "- Basic Python knowledge\n",
    "- Understanding of what LLMs are\n",
    "- Curiosity and enthusiasm! üéâ\n",
    "\n",
    "Let's begin our journey! üöÄ\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## üì¶ Installation and Setup\n",
    "\n",
    "First, let's install the required packages. Run the cell below:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install --quiet langchain-openai langchain-community langchain-core python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain, SimpleSequentialChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.schema import HumanMessage, SystemMessage\n",
    "\n",
    "# Load environment variables (you'll need to create a .env file with your OpenAI API key)\n",
    "load_dotenv()\n",
    "\n",
    "print(\"‚úÖ All imports successful!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up your OpenAI API key (make sure you have it in your .env file)\n",
    "# Create a .env file in your project directory with: OPENAI_API_KEY=your_api_key_here\n",
    "\n",
    "# Initialize the OpenAI LLM with all possible parameters\n",
    "llm_example = OpenAI(\n",
    "    # Core parameters\n",
    "    model_name=\"gpt-3.5-turbo-instruct\",  # The specific model to use (text-davinci-003, gpt-3.5-turbo-instruct, etc.)\n",
    "    temperature=0.7,                       # Controls randomness (0.0 = deterministic, 1.0 = very creative)\n",
    "    max_tokens=256,                        # Maximum number of tokens to generate in the response\n",
    "    top_p=1.0,                            # Nucleus sampling - considers tokens with top_p probability mass\n",
    "    frequency_penalty=0.0,                 # Penalizes frequent tokens (-2.0 to 2.0, reduces repetition)\n",
    "    presence_penalty=0.0,                  # Penalizes new tokens based on presence (-2.0 to 2.0, encourages new topics)\n",
    "    n=1,                                   # Number of completions to generate for each prompt\n",
    "    best_of=1,                            # Generates best_of completions server-side, returns the best one\n",
    "    \n",
    "    # Advanced parameters\n",
    "    logit_bias={},                        # Modify likelihood of specified tokens (token_id: bias_value)\n",
    "    openai_api_key=os.getenv(\"OPENAI_API_KEY\"),  # Your OpenAI API key\n",
    "    openai_organization=None,             # OpenAI organization ID (if applicable)\n",
    "    batch_size=20,                        # Batch size for requests when processing multiple prompts\n",
    "    request_timeout=60,                   # Timeout for API requests in seconds\n",
    "    max_retries=6,                        # Maximum number of retries for failed requests\n",
    "    streaming=False,                      # Whether to stream the response\n",
    "    allowed_special=\"all\",                # Which special tokens are allowed in the input\n",
    "    disallowed_special=\"all\",             # Which special tokens are disallowed in the input\n",
    "    \n",
    "    # Caching and performance\n",
    "    cache=None,                           # Cache implementation for storing results\n",
    "    verbose=False,                        # Whether to print verbose output\n",
    "    callbacks=None,                       # List of callback handlers\n",
    "    callback_manager=None,                # Callback manager for handling callbacks\n",
    "    tags=None,                           # Tags for tracking and organizing requests\n",
    "    metadata=None,                       # Additional metadata for the LLM\n",
    ")\n",
    "\n",
    "print(\"üéâ LLM initialized with all parameters!\")\n",
    "print(f\"Model: {llm.model_name}\")\n",
    "print(f\"Temperature: {llm.temperature}\")\n",
    "print(f\"Max tokens: {llm.max_tokens}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(\n",
    "    model_name=\"gpt-4.1-nano-2025-04-14\",  \n",
    "    temperature=0,                       \n",
    "    max_tokens=256)\n",
    "\n",
    "print(\"üéâ LLM initialized with all parameters!\")\n",
    "print(f\"Model: {llm.model_name}\")\n",
    "print(f\"Temperature: {llm.temperature}\")\n",
    "print(f\"Max tokens: {llm.max_tokens}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## ü§î What is LangChain?\n",
    "\n",
    "**LangChain** is like a toolkit for building AI applications. Think of it as LEGO blocks for AI! üß±\n",
    "\n",
    "### Why LangChain?\n",
    "\n",
    "1. **Modularity**: Build complex AI applications from simple components\n",
    "2. **Flexibility**: Work with different LLM providers (OpenAI, Anthropic, etc.)\n",
    "3. **Memory**: Maintain context across conversations\n",
    "4. **Chains**: Connect multiple AI operations together\n",
    "5. **Tools**: Integrate with external data sources and APIs\n",
    "\n",
    "### üîß Core Concepts\n",
    "\n",
    "| Component | Description | Example |\n",
    "|-----------|-------------|---------| \n",
    "| **LLMs** | Language models that generate text | GPT-3.5, GPT-4 |\n",
    "| **Prompts** | Instructions for the LLM | \"Explain quantum physics in simple terms\" |\n",
    "| **Chains** | Sequences of operations | Question ‚Üí Answer ‚Üí Summary |\n",
    "| **Memory** | Storing conversation history | Remembering previous questions |\n",
    "| **Tools** | External integrations | Web search, calculator, database |\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Quick Question\n",
    "\n",
    "**Think about this**: If you wanted to build a chatbot that can remember your name and preferences, which LangChain components would you need?\n",
    "\n",
    "Write your answer below:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "**Your Answer:** [Write your thoughts here]\n",
    "\n",
    "**Hint**: Consider what components would handle the conversation, store information, and generate responses.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## üéØ Section 1: Your First LangChain Application\n",
    "\n",
    "Let's start with the simplest possible example - a basic text generator!\n",
    "\n",
    "`llm = OpenAI(temperature=0.7)`\n",
    "\n",
    "Above line of code in this notebook imports the OpenAI class from `langchain.llms` and creates an instance without specifying a particular model name. \n",
    "When no model is explicitly specified, the OpenAI class in LangChain <u>defaults</u> to using \"text-davinci-003\" (which is **GPT-3.5**).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "response format:\n",
    "\n",
    "responce_parameter_name = llm_parameter_name.call_llm_function('prompt')\n",
    "\n",
    "call_llm_function: invoke or predicr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Initialize the LLM\n",
    "# Note: You'll need an OpenAI API key in your .env file\n",
    "# Create a .env file with: OPENAI_API_KEY=your_api_key_here\n",
    "\n",
    "try:\n",
    "    # Initialize the language model\n",
    "    llm = ChatOpenAI(\n",
    "    model_name=\"gpt-4.1-nano-2025-04-14\",  \n",
    "    temperature=0,                       \n",
    "    max_tokens=256)  \n",
    "    \n",
    "    # Test the LLM\n",
    "    response = llm.invoke(\"Tell me a short joke about programming\") #invoke\n",
    "    print(\"ü§ñ LLM Response:\")\n",
    "    print(response)\n",
    "    \n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error: {e}\")\n",
    "    print(\"üí° Make sure you have set up your OpenAI API key in a .env file\")\n",
    "    print(\"üìù For now, let's continue with the concepts!\")\n",
    "    \n",
    "    # Simulated response for demonstration\n",
    "    print(\"\\nü§ñ Simulated LLM Response:\")\n",
    "    print(\"Why do programmers prefer dark mode? Because light attracts bugs! üêõ\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "### üéØ Understanding the Code\n",
    "\n",
    "Let's break down what we just did:\n",
    "\n",
    "1. **`OpenAI(temperature=0.7)`**: Creates a connection to OpenAI's language model\n",
    "   - `temperature=0.7`: Controls randomness (0.0 = very predictable, 1.0 = very creative)\n",
    "\n",
    "2. **`llm.predict()`**: Sends a prompt to the LLM and gets a response\n",
    "\n",
    "### üîß Key Parameters\n",
    "\n",
    "- **Temperature**: Controls creativity\n",
    "- **Max Tokens**: Limits response length\n",
    "- **Model**: Which LLM to use (e.g., 'gpt-3.5-turbo', 'gpt-4')\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Quick Exercise\n",
    "\n",
    "**Try this**: What would happen if you changed the temperature to 0.0? What about 1.0?\n",
    "\n",
    "Write your prediction below:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "**Your Prediction:**\n",
    "- Temperature 0.0: [Your guess]\n",
    "- Temperature 1.0: [Your guess]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## üéØ Section 2: Working with Prompts\n",
    "\n",
    "Prompts are like instructions for the LLM. Let's learn how to create effective prompts!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 1: Simple string prompt\n",
    "simple_prompt = \"Explain what machine learning is in one sentence.\"\n",
    "\n",
    "print(\"üìù Simple Prompt:\")\n",
    "print(simple_prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your simple prompt here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 2: PromptTemplate (more powerful)\n",
    "template = PromptTemplate(\n",
    "    input_variables=[\"topic\", \"audience\", \"length\"],\n",
    "    template=\"Explain {topic} to a {audience} in {length} sentences.\"\n",
    ")\n",
    "\n",
    "print(\"üìù Prompt Template:\")\n",
    "print(template.template)\n",
    "print(\"\\nüìã Variables:\", template.input_variables)\n",
    "\n",
    "# Example usage\n",
    "formatted_prompt = template.format(\n",
    "    topic=\"artificial intelligence\",\n",
    "    audience=\"a 10 year old child\",\n",
    "    length=\"3\"\n",
    ")\n",
    "\n",
    "print(\"\\nüéØ Formatted Prompt:\")\n",
    "print(formatted_prompt)\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model_name=\"gpt-4.1-nano-2025-04-14\",  \n",
    "    temperature=0,                       \n",
    "    max_tokens=256)  \n",
    "    \n",
    "response = llm.invoke(formatted_prompt)\n",
    "\n",
    "print(\"-\"*50)\n",
    "print(\"ü§ñ LLM Response:\")\n",
    "print(response.content)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your prompt template here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create a more complex prompt template\n",
    "story_template = PromptTemplate(\n",
    "    input_variables=[\"character\", \"setting\", \"genre\"],\n",
    "    template=\"\"\"\n",
    "    Write a short story with the following requirements:\n",
    "    - Main character: {character}\n",
    "    - Setting: {setting}\n",
    "    - Genre: {genre}\n",
    "    - Length: 2-3 paragraphs\n",
    "    - Make it engaging and creative\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "# Create a story prompt\n",
    "story_prompt = story_template.format(\n",
    "    character=\"a robot who loves cooking\",\n",
    "    setting=\"a futuristic kitchen on Mars\",\n",
    "    genre=\"science fiction comedy\"\n",
    ")\n",
    "\n",
    "print(\"ü§ñ Generated Story Prompt:\")\n",
    "print(story_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your complex prompt template here:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "### üéØ Prompt Engineering Tips\n",
    "\n",
    "1. **Be Specific**: Instead of \"Write about AI\", say \"Write a 3-sentence explanation of AI for beginners\"\n",
    "2. **Use Variables**: Make prompts reusable with templates\n",
    "3. **Set Constraints**: Specify length, tone, format\n",
    "4. **Provide Context**: Give the LLM enough information to work with\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Quick Exercise\n",
    "\n",
    "**Create your own prompt template!**\n",
    "\n",
    "Design a prompt template for explaining complex topics. What variables would you include?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "**Your Prompt Template:**\n",
    "\n",
    "```\n",
    "Template: [Write your template here]\n",
    "Variables: [List your variables]\n",
    "```\n",
    "\n",
    "**Example:**\n",
    "```\n",
    "Template: Explain {topic} to a {audience} using {style} language in {length} sentences.\n",
    "Variables: ['topic', 'audience', 'style', 'length']\n",
    "```\n",
    "\n",
    "Step 1: Create a prompt template\n",
    " \n",
    " ```python\n",
    " explain_template = PromptTemplate(\n",
    "     input_variables=[\"concept\"],\n",
    "     template=\"Explain {concept} in simple terms that a beginner can understand.\")\n",
    " ```\n",
    "\n",
    " Step 2: Create a chain\n",
    "```python\n",
    "explain_chain = LLMChain(llm=llm, prompt=explain_template)\n",
    "\n",
    "print(\"üîó Chain Created Successfully!\")\n",
    "print(\"Chain components:\")\n",
    "print(f\"- LLM: {type(llm).__name__}\")\n",
    "print(f\"- Prompt Template: {explain_template.template}\")\n",
    "print(f\"- Input Variables: {explain_template.input_variables}\")\n",
    " ```\n",
    "OUTPUT:\n",
    "\n",
    "üîó Chain Created Successfully!\n",
    "Chain components:\n",
    "- LLM: OpenAI\n",
    "- Prompt Template: Explain {concept} in simple terms that a beginner can understand.\n",
    "- Input Variables: ['concept']\n",
    "\n",
    "\n",
    "Step 3: Use the chain\n",
    "```python\n",
    "concepts = [\"blockchain\", \"machine learning\", \"cloud computing\"]\n",
    "\n",
    "print(\"üîó Running the Chain:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for concept in concepts:\n",
    "    try:\n",
    "        response = explain_chain.invoke(concept)\n",
    "        print(f\"\\nüìö {concept.upper()}:\")\n",
    "        print(response)\n",
    "        print(\"-\"*40)\n",
    "    except:\n",
    "        print(f\"\\nüìö {concept.upper()}:\")\n",
    "        print(f\"[Simulated explanation of {concept} for beginners]\")\n",
    "        print(\"-\"*40)\n",
    "```\n",
    "\n",
    "OUTPUT:\n",
    "\n",
    "Running the Chain:\n",
    "\n",
    "BLOCKCHAIN:\n",
    "\n",
    "Blockchain is like a digital ledger or record book where information is stored in blocks that are linked together in a chain. Each block contains data and a unique code called a hash. Once a block is created, it is added to the chain and cannot be changed. This makes it very secure and reliable. The information on the blockchain is stored on many different computers, so it is decentralized and cannot be controlled by one person or entity. This technology is commonly used for recording and tracking transactions, like buying and selling cryptocurrencies, but it can also be used for other types of data storage and sharing.\n",
    "\n",
    "MACHINE LEARNING:\n",
    "\n",
    "Machine learning is a type of technology that allows computers to learn and make decisions without being explicitly programmed to do so. It involves feeding large amounts of data into a computer and using algorithms to analyze and find patterns within the data. The computer then uses these patterns to make predictions or decisions about new data it receives. It is like teaching a child how to ride a bike - at first, you show them how to do it, but eventually, they learn on their own and can ride without your help. Similarly, machine learning algorithms learn from the data they are given and improve over time, making them useful for tasks like predicting stock prices, recognizing faces in photos, and even diagnosing diseases.\n",
    "\n",
    "\n",
    "CLOUD COMPUTING:\n",
    "\n",
    "Cloud computing refers to the delivery of computing services over the internet. This means that instead of storing data or running programs on your own computer, you can access them from a remote server through the internet. This allows you to use and store data on the cloud without needing to have physical hardware or infrastructure. It is like renting storage space or using software on someone else's computer, but you can access it from anywhere as long as you have an internet connection. This makes it easier and more convenient to use technology without having to worry about maintenance, updates, or storage space on your own device.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## üéØ Section 3: Building Your First Chain\n",
    "\n",
    "Chains are like assembly lines for AI operations. Let's build one!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "llm = ChatOpenAI(\n",
    "    model_name=\"gpt-4.1-nano-2025-04-14\",  \n",
    "    temperature=0,                       \n",
    "    max_tokens=256)  \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Create a prompt template\n",
    "explain_template = PromptTemplate(\n",
    "    input_variables=[\"ADD HERE\"],\n",
    "    template=\"ADD YOUR TEMPLATE HERE, USE {} FOR THE INPUT VARIABLES.\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 2: PromptTemplate (more powerful)\n",
    "coding_template  = PromptTemplate(\n",
    "    input_variables=[\"programming_language\"],\n",
    "    template=\"Write 'hello world' using {programming_language}.\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Create a chain\n",
    "explain_chain = LLMChain(llm=llm, prompt=coding_template)\n",
    "\n",
    "print(\"üîó Chain Created Successfully!\")\n",
    "print(\"Chain components:\")\n",
    "print(f\"- LLM: {type(llm).__name__}\")\n",
    "print(f\"- Prompt Template: {coding_template.template}\")\n",
    "print(f\"- Input Variables: {coding_template.input_variables}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Use the chain\n",
    "concepts = ['python', 'java', 'javascript', 'assembly']\n",
    "\n",
    "print(\"üîó Running the Chain:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for concept in concepts:\n",
    "    try:\n",
    "        response = explain_chain.invoke(concept)\n",
    "        print(f\"\\nüìö {concept.upper()}:\")\n",
    "        print(response)\n",
    "        print(\"-\"*40)\n",
    "    except:\n",
    "        print(f\"\\nüìö {concept.upper()}:\")\n",
    "        print(f\"[Simulated explanation of {concept} for beginners]\")\n",
    "        print(\"-\"*40)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Use the chain\n",
    "\n",
    "# Method 2: PromptTemplate (more powerful)\n",
    "car_parts = PromptTemplate(\n",
    "    input_variables=[\"car_parts\"],\n",
    "    template=\"explain the mechanics of {car_parts} in 1 sentence. keep it short and simple.\"\n",
    ")\n",
    "\n",
    "concepts = ['car engine', 'car transmission', 'car brakes']\n",
    "\n",
    "explain_chain = LLMChain(llm=llm, prompt=car_parts)\n",
    "\n",
    "\n",
    "print(\"üîó Running the Chain:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for concept in concepts:\n",
    "    try:\n",
    "        response = explain_chain.invoke(concept)\n",
    "        print(f\"\\nüìö {concept.upper()}:\")\n",
    "        print(response)\n",
    "        print(\"-\"*40)\n",
    "    except:\n",
    "        print(f\"\\nüìö {concept.upper()}:\")\n",
    "        print(f\"[Simulated explanation of {concept} for beginners]\")\n",
    "        print(\"-\"*40)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create a more complex chain - a question generator\n",
    "question_template = PromptTemplate(\n",
    "    input_variables=[\"topic\", \"difficulty\"],\n",
    "    template=\"\"\"\n",
    "    Generate 3 {difficulty} questions about {topic}.\n",
    "    Format each question on a new line starting with 'Q: '\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "question_chain = LLMChain(llm=llm, prompt=question_template)\n",
    "\n",
    "print(\"üéØ Question Generator Chain:\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "try:\n",
    "    questions = question_chain.invoke({\"topic\": \"Python programming\", \"difficulty\": \"beginner\"})\n",
    "    print(questions)\n",
    "except:\n",
    "    print(\"Q: What is a variable in Python?\") \n",
    "    print(\"Q: How do you print 'Hello World' in Python?\") \n",
    "    print(\"Q: What is the difference between a list and a tuple?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR COMPLEX CHAIN HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "### üîó Understanding Chains\n",
    "\n",
    "**Chains** connect multiple operations together:\n",
    "\n",
    "```\n",
    "Input ‚Üí Prompt Template ‚Üí LLM ‚Üí Output\n",
    "```\n",
    "\n",
    "**Benefits of Chains:**\n",
    "- Reusable components\n",
    "- Consistent formatting\n",
    "- Easy to modify and extend\n",
    "- Can be combined with other chains\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Quick Exercise\n",
    "\n",
    "**Design a Chain**: What kind of chain would you create for a study assistant? What would the input and output be?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "**Your Study Assistant Chain Design:**\n",
    "\n",
    "**Input:** [What would the user provide?]\n",
    "**Output:** [What would the chain produce?]\n",
    "**Template:** [What would the prompt template look like?]\n",
    "\n",
    "**Example:**\n",
    "- **Input:** Topic (e.g., \"photosynthesis\")\n",
    "- **Output:** Study guide with key points\n",
    "- **Template:** \"Create a study guide for {topic} with main concepts and examples\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## üéØ Section 4: Memory and Context\n",
    "\n",
    "Memory allows your AI to remember previous conversations. This is crucial for chatbots!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a conversation memory\n",
    "memory = ConversationBufferMemory()\n",
    "\n",
    "print(\"üß† Memory System Created!\")\n",
    "print(f\"Memory type: {type(memory).__name__}\")\n",
    "print(f\"Current memory: {memory.buffer}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a conversation chain with memory\n",
    "conversation_template = PromptTemplate(\n",
    "    input_variables=[\"history\", \"human_input\"],\n",
    "    template=\"\"\"\n",
    "    You are a helpful AI assistant. Use the conversation history to provide context-aware responses.\n",
    "    \n",
    "    Conversation History:\n",
    "    {history}\n",
    "    \n",
    "    Human: {human_input}\n",
    "    AI: \"\"\"\n",
    ")\n",
    "\n",
    "conversation_chain = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=conversation_template,\n",
    "    memory=memory,\n",
    "    verbose=True  # Shows the chain's thinking process\n",
    ")\n",
    "\n",
    "print(\"üí¨ Conversation Chain with Memory Created!\")\n",
    "print(\"=\"*60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate a conversation\n",
    "conversation_messages = [\n",
    "    \"Hi! My name is Alex.\",\n",
    "    \"I'm learning Python programming.\",\n",
    "    \"What should I study first?\",\n",
    "    \"Can you remind me what my name is?\"\n",
    "]\n",
    "\n",
    "print(\"üí¨ Simulated Conversation:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for i, message in enumerate(conversation_messages, 1):\n",
    "    print(f\"\\nüë§ Human {i}: {message}\")\n",
    "    \n",
    "    try:\n",
    "        response = conversation_chain.run(message)\n",
    "        print(f\"ü§ñ AI {i}: {response}\")\n",
    "    except:\n",
    "        # Simulated responses\n",
    "        if i == 1:\n",
    "            print(\"ü§ñ AI 1: Hello Alex! Nice to meet you! How can I help you today?\")\n",
    "        elif i == 2:\n",
    "            print(\"ü§ñ AI 2: That's great, Alex! Python is an excellent choice for beginners.\")\n",
    "        elif i == 3:\n",
    "            print(\"ü§ñ AI 3: For Python beginners, I'd recommend starting with variables, data types, and basic syntax.\")\n",
    "        elif i == 4:\n",
    "            print(\"ü§ñ AI 4: Your name is Alex! You mentioned it at the beginning of our conversation.\")\n",
    "    \n",
    "    print(\"-\"*40)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check what's stored in memory\n",
    "print(\"üß† Current Memory Contents:\")\n",
    "print(\"=\"*40)\n",
    "print(memory.buffer)\n",
    "\n",
    "print(\"\\nüìä Memory Statistics:\")\n",
    "print(f\"- Total messages: {len(memory.chat_memory.messages)}\")\n",
    "print(f\"- Memory type: {type(memory).__name__}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "### üß† Types of Memory in LangChain\n",
    "\n",
    "1. **ConversationBufferMemory**: Stores all messages\n",
    "2. **ConversationSummaryMemory**: Summarizes long conversations\n",
    "3. **ConversationTokenBufferMemory**: Limits memory by token count\n",
    "4. **ConversationBufferWindowMemory**: Keeps only recent messages\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Quick Exercise\n",
    "\n",
    "**Memory Challenge**: If you were building a customer service chatbot, which type of memory would you choose and why?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "**Your Answer:**\n",
    "\n",
    "**Memory Type:** [Your choice]\n",
    "**Reason:** [Why you chose this type]\n",
    "\n",
    "**Hint:** Consider factors like conversation length, privacy, and performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## üéØ Section 5: Real-World Applications\n",
    "\n",
    "Let's build some practical examples that you might actually use!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Application 1: Code Review Assistant\n",
    "code_review_template = PromptTemplate(\n",
    "    input_variables=[\"code\", \"language\"],\n",
    "    template=\"\"\"\n",
    "    Review this {language} code and provide feedback:\n",
    "    \n",
    "    Code:\n",
    "    {code}\n",
    "    \n",
    "    Please provide:\n",
    "    1. What the code does\n",
    "    2. Potential improvements\n",
    "    3. Best practices suggestions\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "code_review_chain = LLMChain(llm=llm, prompt=code_review_template)\n",
    "\n",
    "print(\"üîç Code Review Assistant Created!\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Sample code to review\n",
    "sample_code = \"\"\"\n",
    "def calculate_average(numbers):\n",
    "    total = 0\n",
    "    for num in numbers:\n",
    "        total = total + num\n",
    "    return total / len(numbers)\n",
    "\"\"\"\n",
    "\n",
    "try:\n",
    "    review = code_review_chain.run({\"code\": sample_code, \"language\": \"Python\"})\n",
    "    print(\"\\nüìù Code Review:\")\n",
    "    print(review)\n",
    "except:\n",
    "    print(\"\\nüìù Simulated Code Review:\")\n",
    "    print(\"\"\"\n",
    "    1. What the code does: Calculates the average of a list of numbers\n",
    "    \n",
    "    2. Potential improvements:\n",
    "       - Add input validation for empty lists\n",
    "       - Use sum() function instead of manual loop\n",
    "       - Add type hints\n",
    "    \n",
    "    3. Best practices suggestions:\n",
    "       - Handle division by zero\n",
    "       - Add docstring\n",
    "       - Consider using statistics.mean()\n",
    "    \"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Application 2: Study Guide Generator\n",
    "study_guide_template = PromptTemplate(\n",
    "    input_variables=[\"topic\", \"level\"],\n",
    "    template=\"\"\"\n",
    "    Create a comprehensive study guide for {topic} at {level} level.\n",
    "    \n",
    "    Include:\n",
    "    1. Key concepts and definitions\n",
    "    2. Important examples\n",
    "    3. Common mistakes to avoid\n",
    "    4. Practice questions\n",
    "    5. Additional resources\n",
    "    \n",
    "    Format it clearly with headers and bullet points.\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "study_guide_chain = LLMChain(llm=llm, prompt=study_guide_template)\n",
    "\n",
    "print(\"üìö Study Guide Generator Created!\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "try:\n",
    "    guide = study_guide_chain.run({\"topic\": \"functions in programming\", \"level\": \"beginner\"})\n",
    "    print(\"\\nüìñ Study Guide:\")\n",
    "    print(guide)\n",
    "except:\n",
    "    print(\"\\nüìñ Simulated Study Guide:\")\n",
    "    print(\"\"\"\n",
    "    # Functions in Programming - Beginner Guide\n",
    "    \n",
    "    ## Key Concepts\n",
    "    - Functions are reusable blocks of code\n",
    "    - They take inputs (parameters) and return outputs\n",
    "    - They help organize and modularize code\n",
    "    \n",
    "    ## Important Examples\n",
    "    ```python\n",
    "    def greet(name):\n",
    "        return f\"Hello, {name}!\"\n",
    "    ```\n",
    "    \n",
    "    ## Common Mistakes\n",
    "    - Forgetting to call the function\n",
    "    - Not returning values when needed\n",
    "    - Incorrect parameter order\n",
    "    \n",
    "    ## Practice Questions\n",
    "    1. What is the difference between parameters and arguments?\n",
    "    2. How do you create a function that returns multiple values?\n",
    "    \n",
    "    ## Additional Resources\n",
    "    - Python documentation on functions\n",
    "    - Online tutorials and practice exercises\n",
    "    \"\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## üéØ Section 6: Advanced Concepts (Preview)\n",
    "\n",
    "Here's a glimpse of what you can do with more advanced LangChain features:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sequential Chains - Multiple operations in sequence\n",
    "print(\"üîó Sequential Chain Example:\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "# Step 1: Generate a topic\n",
    "topic_template = PromptTemplate(\n",
    "    input_variables=[\"interest\"],\n",
    "    template=\"Generate an interesting topic related to {interest}.\"\n",
    ")\n",
    "\n",
    "# Step 2: Create questions about that topic\n",
    "question_template = PromptTemplate(\n",
    "    input_variables=[\"topic\"],\n",
    "    template=\"Create 3 questions about {topic}.\"\n",
    ")\n",
    "\n",
    "# Step 3: Answer those questions\n",
    "answer_template = PromptTemplate(\n",
    "    input_variables=[\"topic\", \"questions\"],\n",
    "    template=\"\"\"\n",
    "    Topic: {topic}\n",
    "    Questions: {questions}\n",
    "    \n",
    "    Provide detailed answers to each question.\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "print(\"üìã Chain Flow:\")\n",
    "print(\"Interest ‚Üí Topic ‚Üí Questions ‚Üí Answers\")\n",
    "print(\"\\nüéØ This creates a complete learning module automatically!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## üéØ Final Challenge\n",
    "\n",
    "**Design Your Own LangChain Application!**\n",
    "\n",
    "Think about a problem you'd like to solve with AI. Design a LangChain application for it:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "**Your Application Design:**\n",
    "\n",
    "**Application Name:** [Your idea]\n",
    "\n",
    "**Problem it solves:** [What problem does it address?]\n",
    "\n",
    "**Components needed:**\n",
    "- LLM: [Which model?]\n",
    "- Prompts: [What prompts would you use?]\n",
    "- Chains: [How would you structure the flow?]\n",
    "- Memory: [Would it need memory?]\n",
    "- Tools: [Any external integrations?]\n",
    "\n",
    "**Example:**\n",
    "- **Name:** Personal Fitness Coach\n",
    "- **Problem:** Helps users create workout plans\n",
    "- **Components:** LLM for advice, prompts for workout generation, memory for tracking progress, tools for exercise database\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## üéâ Congratulations!\n",
    "\n",
    "You've completed your introduction to LangChain! Here's what you've learned:\n",
    "\n",
    "### ‚úÖ Key Takeaways\n",
    "\n",
    "1. **LangChain Basics**: Understanding the framework and its components\n",
    "2. **LLMs and Prompts**: How to work with language models effectively\n",
    "3. **Chains**: Building reusable AI workflows\n",
    "4. **Memory**: Maintaining context in conversations\n",
    "5. **Real Applications**: Practical examples you can build upon\n",
    "\n",
    "### üöÄ Next Steps\n",
    "\n",
    "1. **Practice**: Try building your own chains and applications\n",
    "2. **Explore**: Check out LangChain's documentation and examples\n",
    "3. **Experiment**: Combine different components to create unique solutions\n",
    "4. **Learn More**: Dive into advanced topics like agents, tools, and embeddings\n",
    "\n",
    "### üìö Resources\n",
    "\n",
    "- [LangChain Documentation](https://python.langchain.com/)\n",
    "- [LangChain GitHub](https://github.com/langchain-ai/langchain)\n",
    "- [OpenAI API Documentation](https://platform.openai.com/docs)\n",
    "- [Prompt Engineering Guide](https://www.promptingguide.ai/)\n",
    "\n",
    "### üéØ Remember\n",
    "\n",
    "LangChain is like a toolkit - the more you practice, the more powerful your AI applications become! Start small, experiment often, and don't be afraid to make mistakes. Every great AI application started with a simple chain. üöÄ\n",
    "\n",
    "**Happy coding!** üíª‚ú®\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## üìù Additional Exercises\n",
    "\n",
    "Try these exercises to reinforce your learning:\n",
    "\n",
    "### Exercise 1: Email Generator\n",
    "Create a chain that generates professional emails based on the recipient and purpose.\n",
    "\n",
    "### Exercise 2: Language Translator\n",
    "Build a chain that translates text between different languages.\n",
    "\n",
    "### Exercise 3: Quiz Generator\n",
    "Create a system that generates quizzes on any topic with multiple-choice questions.\n",
    "\n",
    "### Exercise 4: Code Debugger\n",
    "Build a chain that helps debug Python code by analyzing error messages.\n",
    "\n",
    "### Exercise 5: Personal Assistant\n",
    "Design a conversational assistant that can help with daily tasks and remember user preferences.\n",
    "\n",
    "---\n",
    "\n",
    "**Share your creations and discoveries with the community!** üåü\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
