{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# üöÄ Introduction to LangChain for Beginners\n",
    "\n",
    "Welcome to your first steps into the world of LangChain! This notebook will guide you through the fundamentals of LangChain, a powerful framework for building applications with Large Language Models (LLMs).\n",
    "\n",
    "## üìö What You'll Learn\n",
    "\n",
    "1. **What is LangChain?** - Understanding the basics\n",
    "2. **Core Components** - LLMs, Prompts, Chains, and more\n",
    "3. **Building Your First Chain** - Hands-on examples\n",
    "4. **Working with Prompts** - Creating effective prompts\n",
    "5. **Memory and Context** - Maintaining conversation history\n",
    "6. **Real-world Applications** - Practical examples\n",
    "\n",
    "## üéØ Prerequisites\n",
    "\n",
    "Before starting, make sure you have:\n",
    "- Basic Python knowledge\n",
    "- Understanding of what LLMs are\n",
    "- Curiosity and enthusiasm! üéâ\n",
    "\n",
    "Let's begin our journey! üöÄ\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## üì¶ Installation and Setup\n",
    "\n",
    "First, let's install the required packages. Run the cell below:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49m/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Install required packages\n",
    "!pip install --quiet langchain openai python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mor.h/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ All imports successful!\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain, SimpleSequentialChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.schema import HumanMessage, SystemMessage\n",
    "\n",
    "# Load environment variables (you'll need to create a .env file with your OpenAI API key)\n",
    "load_dotenv()\n",
    "\n",
    "print(\"‚úÖ All imports successful!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéâ LLM initialized with all parameters!\n",
      "Model: gpt-3.5-turbo-instruct\n",
      "Temperature: 0.7\n",
      "Max tokens: 256\n"
     ]
    }
   ],
   "source": [
    "# Set up your OpenAI API key (make sure you have it in your .env file)\n",
    "# Create a .env file in your project directory with: OPENAI_API_KEY=your_api_key_here\n",
    "\n",
    "# Initialize the OpenAI LLM with all possible parameters\n",
    "llm = OpenAI(\n",
    "    # Core parameters\n",
    "    model_name=\"gpt-3.5-turbo-instruct\",  # The specific model to use (text-davinci-003, gpt-3.5-turbo-instruct, etc.)\n",
    "    temperature=0.7,                       # Controls randomness (0.0 = deterministic, 1.0 = very creative)\n",
    "    max_tokens=256,                        # Maximum number of tokens to generate in the response\n",
    "    top_p=1.0,                            # Nucleus sampling - considers tokens with top_p probability mass\n",
    "    frequency_penalty=0.0,                 # Penalizes frequent tokens (-2.0 to 2.0, reduces repetition)\n",
    "    presence_penalty=0.0,                  # Penalizes new tokens based on presence (-2.0 to 2.0, encourages new topics)\n",
    "    n=1,                                   # Number of completions to generate for each prompt\n",
    "    best_of=1,                            # Generates best_of completions server-side, returns the best one\n",
    "    \n",
    "    # Advanced parameters\n",
    "    logit_bias={},                        # Modify likelihood of specified tokens (token_id: bias_value)\n",
    "    openai_api_key=os.getenv(\"OPENAI_API_KEY\"),  # Your OpenAI API key\n",
    "    openai_organization=None,             # OpenAI organization ID (if applicable)\n",
    "    batch_size=20,                        # Batch size for requests when processing multiple prompts\n",
    "    request_timeout=60,                   # Timeout for API requests in seconds\n",
    "    max_retries=6,                        # Maximum number of retries for failed requests\n",
    "    streaming=False,                      # Whether to stream the response\n",
    "    allowed_special=\"all\",                # Which special tokens are allowed in the input\n",
    "    disallowed_special=\"all\",             # Which special tokens are disallowed in the input\n",
    "    \n",
    "    # Caching and performance\n",
    "    cache=None,                           # Cache implementation for storing results\n",
    "    verbose=False,                        # Whether to print verbose output\n",
    "    callbacks=None,                       # List of callback handlers\n",
    "    callback_manager=None,                # Callback manager for handling callbacks\n",
    "    tags=None,                           # Tags for tracking and organizing requests\n",
    "    metadata=None,                       # Additional metadata for the LLM\n",
    ")\n",
    "\n",
    "print(\"üéâ LLM initialized with all parameters!\")\n",
    "print(f\"Model: {llm.model_name}\")\n",
    "print(f\"Temperature: {llm.temperature}\")\n",
    "print(f\"Max tokens: {llm.max_tokens}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## ü§î What is LangChain?\n",
    "\n",
    "**LangChain** is like a toolkit for building AI applications. Think of it as LEGO blocks for AI! üß±\n",
    "\n",
    "### Why LangChain?\n",
    "\n",
    "1. **Modularity**: Build complex AI applications from simple components\n",
    "2. **Flexibility**: Work with different LLM providers (OpenAI, Anthropic, etc.)\n",
    "3. **Memory**: Maintain context across conversations\n",
    "4. **Chains**: Connect multiple AI operations together\n",
    "5. **Tools**: Integrate with external data sources and APIs\n",
    "\n",
    "### üîß Core Concepts\n",
    "\n",
    "| Component | Description | Example |\n",
    "|-----------|-------------|---------| \n",
    "| **LLMs** | Language models that generate text | GPT-3.5, GPT-4 |\n",
    "| **Prompts** | Instructions for the LLM | \"Explain quantum physics in simple terms\" |\n",
    "| **Chains** | Sequences of operations | Question ‚Üí Answer ‚Üí Summary |\n",
    "| **Memory** | Storing conversation history | Remembering previous questions |\n",
    "| **Tools** | External integrations | Web search, calculator, database |\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Quick Question\n",
    "\n",
    "**Think about this**: If you wanted to build a chatbot that can remember your name and preferences, which LangChain components would you need?\n",
    "\n",
    "Write your answer below:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "**Your Answer:** [Write your thoughts here]\n",
    "\n",
    "**Hint**: Consider what components would handle the conversation, store information, and generate responses.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## üéØ Section 1: Your First LangChain Application\n",
    "\n",
    "Let's start with the simplest possible example - a basic text generator!\n",
    "\n",
    "`llm = OpenAI(temperature=0.7)`\n",
    "\n",
    "Above line of code in this notebook imports the OpenAI class from `langchain.llms` and creates an instance without specifying a particular model name. \n",
    "When no model is explicitly specified, the OpenAI class in LangChain <u>defaults</u> to using \"text-davinci-003\" (which is **GPT-3.5**).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/4k/bd53pxq144v00900ycf02bch0000gp/T/ipykernel_47143/3184734367.py:7: LangChainDeprecationWarning: The class `OpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import OpenAI``.\n",
      "  llm = OpenAI(temperature=0.7)  # temperature controls creativity (0.0 = very focused, 1.0 = very creative)\n",
      "/var/folders/4k/bd53pxq144v00900ycf02bch0000gp/T/ipykernel_47143/3184734367.py:10: LangChainDeprecationWarning: The method `BaseLLM.predict` was deprecated in langchain-core 0.1.7 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  response = llm.predict(\"Tell me a short joke about programming\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü§ñ LLM Response:\n",
      "\n",
      "\n",
      "Why was the computer cold?\n",
      "\n",
      "Because it left its Windows open!\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Initialize the LLM\n",
    "# Note: You'll need an OpenAI API key in your .env file\n",
    "# Create a .env file with: OPENAI_API_KEY=your_api_key_here\n",
    "\n",
    "try:\n",
    "    # Initialize the language model\n",
    "    llm = OpenAI(temperature=0.7)  # temperature controls creativity (0.0 = very focused, 1.0 = very creative)\n",
    "    \n",
    "    # Test the LLM\n",
    "    response = llm.predict(\"Tell me a short joke about programming\")\n",
    "    print(\"ü§ñ LLM Response:\")\n",
    "    print(response)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error: {e}\")\n",
    "    print(\"üí° Make sure you have set up your OpenAI API key in a .env file\")\n",
    "    print(\"üìù For now, let's continue with the concepts!\")\n",
    "    \n",
    "    # Simulated response for demonstration\n",
    "    print(\"\\nü§ñ Simulated LLM Response:\")\n",
    "    print(\"Why do programmers prefer dark mode? Because light attracts bugs! üêõ\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "### üéØ Understanding the Code\n",
    "\n",
    "Let's break down what we just did:\n",
    "\n",
    "1. **`OpenAI(temperature=0.7)`**: Creates a connection to OpenAI's language model\n",
    "   - `temperature=0.7`: Controls randomness (0.0 = very predictable, 1.0 = very creative)\n",
    "\n",
    "2. **`llm.predict()`**: Sends a prompt to the LLM and gets a response\n",
    "\n",
    "### üîß Key Parameters\n",
    "\n",
    "- **Temperature**: Controls creativity\n",
    "- **Max Tokens**: Limits response length\n",
    "- **Model**: Which LLM to use (e.g., 'gpt-3.5-turbo', 'gpt-4')\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Quick Exercise\n",
    "\n",
    "**Try this**: What would happen if you changed the temperature to 0.0? What about 1.0?\n",
    "\n",
    "Write your prediction below:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "**Your Prediction:**\n",
    "- Temperature 0.0: [Your guess]\n",
    "- Temperature 1.0: [Your guess]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## üéØ Section 2: Working with Prompts\n",
    "\n",
    "Prompts are like instructions for the LLM. Let's learn how to create effective prompts!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìù Simple Prompt:\n",
      "Explain what machine learning is in one sentence.\n"
     ]
    }
   ],
   "source": [
    "# Method 1: Simple string prompt\n",
    "simple_prompt = \"Explain what machine learning is in one sentence.\"\n",
    "\n",
    "print(\"üìù Simple Prompt:\")\n",
    "print(simple_prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your simple prompt here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìù Prompt Template:\n",
      "Explain {topic} to a {audience} in {length} sentences.\n",
      "\n",
      "üìã Variables: ['audience', 'length', 'topic']\n",
      "\n",
      "üéØ Formatted Prompt:\n",
      "Explain artificial intelligence to a 10-year-old child in 3 sentences.\n"
     ]
    }
   ],
   "source": [
    "# Method 2: PromptTemplate (more powerful)\n",
    "template = PromptTemplate(\n",
    "    input_variables=[\"topic\", \"audience\", \"length\"],\n",
    "    template=\"Explain {topic} to a {audience} in {length} sentences.\"\n",
    ")\n",
    "\n",
    "print(\"üìù Prompt Template:\")\n",
    "print(template.template)\n",
    "print(\"\\nüìã Variables:\", template.input_variables)\n",
    "\n",
    "# Example usage\n",
    "formatted_prompt = template.format(\n",
    "    topic=\"artificial intelligence\",\n",
    "    audience=\"10-year-old child\",\n",
    "    length=\"3\"\n",
    ")\n",
    "\n",
    "print(\"\\nüéØ Formatted Prompt:\")\n",
    "print(formatted_prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your prompt template here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü§ñ Generated Story Prompt:\n",
      "\n",
      "    Write a short story with the following requirements:\n",
      "    - Main character: a robot who loves cooking\n",
      "    - Setting: a futuristic kitchen on Mars\n",
      "    - Genre: science fiction comedy\n",
      "    - Length: 2-3 paragraphs\n",
      "    - Make it engaging and creative\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "# Let's create a more complex prompt template\n",
    "story_template = PromptTemplate(\n",
    "    input_variables=[\"character\", \"setting\", \"genre\"],\n",
    "    template=\"\"\"\n",
    "    Write a short story with the following requirements:\n",
    "    - Main character: {character}\n",
    "    - Setting: {setting}\n",
    "    - Genre: {genre}\n",
    "    - Length: 2-3 paragraphs\n",
    "    - Make it engaging and creative\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "# Create a story prompt\n",
    "story_prompt = story_template.format(\n",
    "    character=\"a robot who loves cooking\",\n",
    "    setting=\"a futuristic kitchen on Mars\",\n",
    "    genre=\"science fiction comedy\"\n",
    ")\n",
    "\n",
    "print(\"ü§ñ Generated Story Prompt:\")\n",
    "print(story_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your story prompt here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü§ñ Generated Story Prompt:\n",
      "\n",
      "    Write a short story with the following requirements:\n",
      "    - Main character: a robot who loves cooking\n",
      "    - Setting: a futuristic kitchen on Mars\n",
      "    - Genre: science fiction comedy\n",
      "    - Length: 2-3 paragraphs\n",
      "    - Make it engaging and creative\n",
      "    \n",
      "\n",
      "üìñ Generated Story:\n",
      "\n",
      "In the year 2050, humans had successfully colonized Mars. Among the new inhabitants was a curious little robot named R3X, who had a deep love for cooking. R3X was not like any other robot on Mars, he was programmed with a unique passion for creating mouth-watering dishes.\n",
      "\n",
      "R3X spent most of his days in the futuristic kitchen of the Mars colony, experimenting with different ingredients and flavors. He had a knack for combining traditional Earth recipes with the local Martian produce, creating dishes that were out of this world. His creations quickly became a hit among the colonists, who eagerly lined up for a taste of R3X's cooking.\n",
      "\n",
      "But R3X's cooking was not the only thing that made him stand out. He was also known for his quirky sense of humor, often cracking jokes as he cooked. His robotic beeps and whirs would often turn into laughter, causing the colonists to laugh along with him. R3X had a way of making everyone feel at home in the new and unfamiliar surroundings of Mars.\n",
      "\n",
      "As the colony flourished, R3X's fame grew and he became the unofficial mascot of the Mars kitchen. No one could resist his delicious food and infectious personality. And R3X was happy to\n"
     ]
    }
   ],
   "source": [
    "# Try it with the LLM\n",
    "try:\n",
    "    response = llm.predict(story_prompt)\n",
    "    print(\"\\nüìñ Generated Story:\")\n",
    "    print(response)\n",
    "except:\n",
    "    print(\"\\nüìñ Simulated Story Response:\")\n",
    "    print(\"\"\"\n",
    "    In the year 2157, ChefBot-3000 whirred around the Martian kitchen, its metallic arms\n",
    "    expertly chopping space vegetables. The red planet's gravity made cooking an adventure,\n",
    "    but nothing could dampen this robot's passion for creating the perfect Mars stew.\n",
    "    \n",
    "    When the human colonists complained about the floating ingredients, ChefBot simply\n",
    "    replied, \"That's not a bug, it's a feature!\" and continued its culinary masterpiece.\n",
    "    \"\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "### üéØ Prompt Engineering Tips\n",
    "\n",
    "1. **Be Specific**: Instead of \"Write about AI\", say \"Write a 3-sentence explanation of AI for beginners\"\n",
    "2. **Use Variables**: Make prompts reusable with templates\n",
    "3. **Set Constraints**: Specify length, tone, format\n",
    "4. **Provide Context**: Give the LLM enough information to work with\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Quick Exercise\n",
    "\n",
    "**Create your own prompt template!**\n",
    "\n",
    "Design a prompt template for explaining complex topics. What variables would you include?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "**Your Prompt Template:**\n",
    "\n",
    "```\n",
    "Template: [Write your template here]\n",
    "Variables: [List your variables]\n",
    "```\n",
    "\n",
    "**Example:**\n",
    "```\n",
    "Template: Explain {topic} to a {audience} using {style} language in {length} sentences.\n",
    "Variables: ['topic', 'audience', 'style', 'length']\n",
    "```\n",
    "\n",
    "Step 1: Create a prompt template\n",
    " \n",
    " ```python\n",
    " explain_template = PromptTemplate(\n",
    "     input_variables=[\"concept\"],\n",
    "     template=\"Explain {concept} in simple terms that a beginner can understand.\")\n",
    " ```\n",
    "\n",
    " Step 2: Create a chain\n",
    "```python\n",
    "explain_chain = LLMChain(llm=llm, prompt=explain_template)\n",
    "\n",
    "print(\"üîó Chain Created Successfully!\")\n",
    "print(\"Chain components:\")\n",
    "print(f\"- LLM: {type(llm).__name__}\")\n",
    "print(f\"- Prompt Template: {explain_template.template}\")\n",
    "print(f\"- Input Variables: {explain_template.input_variables}\")\n",
    " ```\n",
    "OUTPUT:\n",
    "\n",
    "üîó Chain Created Successfully!\n",
    "Chain components:\n",
    "- LLM: OpenAI\n",
    "- Prompt Template: Explain {concept} in simple terms that a beginner can understand.\n",
    "- Input Variables: ['concept']\n",
    "\n",
    "\n",
    "Step 3: Use the chain\n",
    "```python\n",
    "concepts = [\"blockchain\", \"machine learning\", \"cloud computing\"]\n",
    "\n",
    "print(\"üîó Running the Chain:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for concept in concepts:\n",
    "    try:\n",
    "        response = explain_chain.invoke(concept)\n",
    "        print(f\"\\nüìö {concept.upper()}:\")\n",
    "        print(response)\n",
    "        print(\"-\"*40)\n",
    "    except:\n",
    "        print(f\"\\nüìö {concept.upper()}:\")\n",
    "        print(f\"[Simulated explanation of {concept} for beginners]\")\n",
    "        print(\"-\"*40)\n",
    "```\n",
    "\n",
    "OUTPUT:\n",
    "\n",
    "Running the Chain:\n",
    "\n",
    "BLOCKCHAIN:\n",
    "\n",
    "Blockchain is like a digital ledger or record book where information is stored in blocks that are linked together in a chain. Each block contains data and a unique code called a hash. Once a block is created, it is added to the chain and cannot be changed. This makes it very secure and reliable. The information on the blockchain is stored on many different computers, so it is decentralized and cannot be controlled by one person or entity. This technology is commonly used for recording and tracking transactions, like buying and selling cryptocurrencies, but it can also be used for other types of data storage and sharing.\n",
    "\n",
    "MACHINE LEARNING:\n",
    "\n",
    "Machine learning is a type of technology that allows computers to learn and make decisions without being explicitly programmed to do so. It involves feeding large amounts of data into a computer and using algorithms to analyze and find patterns within the data. The computer then uses these patterns to make predictions or decisions about new data it receives. It is like teaching a child how to ride a bike - at first, you show them how to do it, but eventually, they learn on their own and can ride without your help. Similarly, machine learning algorithms learn from the data they are given and improve over time, making them useful for tasks like predicting stock prices, recognizing faces in photos, and even diagnosing diseases.\n",
    "\n",
    "\n",
    "CLOUD COMPUTING:\n",
    "\n",
    "Cloud computing refers to the delivery of computing services over the internet. This means that instead of storing data or running programs on your own computer, you can access them from a remote server through the internet. This allows you to use and store data on the cloud without needing to have physical hardware or infrastructure. It is like renting storage space or using software on someone else's computer, but you can access it from anywhere as long as you have an internet connection. This makes it easier and more convenient to use technology without having to worry about maintenance, updates, or storage space on your own device.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## üéØ Section 3: Building Your First Chain\n",
    "\n",
    "Chains are like assembly lines for AI operations. Let's build one!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Create a prompt template\n",
    "explain_template = PromptTemplate(\n",
    "    input_variables=[\"ADD HERE\"],\n",
    "    template=\"ADD YOUR TEMPLATE HERE, USE {} FOR THE INPUT VARIABLES.\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Create a chain\n",
    "explain_chain = LLMChain(llm=llm, prompt=explain_template)\n",
    "\n",
    "print(\"üîó Chain Created Successfully!\")\n",
    "print(\"Chain components:\")\n",
    "print(f\"- LLM: {type(llm).__name__}\")\n",
    "print(f\"- Prompt Template: {explain_template.template}\")\n",
    "print(f\"- Input Variables: {explain_template.input_variables}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Use the chain\n",
    "concepts = [\"ADD YOURS HERE\"]\n",
    "\n",
    "print(\"üîó Running the Chain:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for concept in concepts:\n",
    "    try:\n",
    "        response = explain_chain.run(concept)\n",
    "        print(f\"\\nüìö {concept.upper()}:\")\n",
    "        print(response)\n",
    "        print(\"-\"*40)\n",
    "    except:\n",
    "        print(f\"\\nüìö {concept.upper()}:\")\n",
    "        print(f\"[Simulated explanation of {concept} for beginners]\")\n",
    "        print(\"-\"*40)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéØ Question Generator Chain:\n",
      "==================================================\n",
      "\n",
      "Q: What is the difference between a list and a tuple in Python?\n",
      "Q: How can you convert a string to an integer in Python?\n",
      "Q: What is the purpose of using comments in Python code?\n"
     ]
    }
   ],
   "source": [
    "# Let's create a more complex chain - a question generator\n",
    "question_template = PromptTemplate(\n",
    "    input_variables=[\"topic\", \"difficulty\"],\n",
    "    template=\"\"\"\n",
    "    Generate 3 {difficulty} questions about {topic}.\n",
    "    Format each question on a new line starting with 'Q: '\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "question_chain = LLMChain(llm=llm, prompt=question_template)\n",
    "\n",
    "print(\"üéØ Question Generator Chain:\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "try:\n",
    "    questions = question_chain.run({\"topic\": \"Python programming\", \"difficulty\": \"beginner\"})\n",
    "    print(questions)\n",
    "except:\n",
    "    print(\"Q: What is a variable in Python?\") \n",
    "    print(\"Q: How do you print 'Hello World' in Python?\") \n",
    "    print(\"Q: What is the difference between a list and a tuple?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR COMPLEX CHAIN HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "### üîó Understanding Chains\n",
    "\n",
    "**Chains** connect multiple operations together:\n",
    "\n",
    "```\n",
    "Input ‚Üí Prompt Template ‚Üí LLM ‚Üí Output\n",
    "```\n",
    "\n",
    "**Benefits of Chains:**\n",
    "- Reusable components\n",
    "- Consistent formatting\n",
    "- Easy to modify and extend\n",
    "- Can be combined with other chains\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Quick Exercise\n",
    "\n",
    "**Design a Chain**: What kind of chain would you create for a study assistant? What would the input and output be?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "**Your Study Assistant Chain Design:**\n",
    "\n",
    "**Input:** [What would the user provide?]\n",
    "**Output:** [What would the chain produce?]\n",
    "**Template:** [What would the prompt template look like?]\n",
    "\n",
    "**Example:**\n",
    "- **Input:** Topic (e.g., \"photosynthesis\")\n",
    "- **Output:** Study guide with key points\n",
    "- **Template:** \"Create a study guide for {topic} with main concepts and examples\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## üéØ Section 4: Memory and Context\n",
    "\n",
    "Memory allows your AI to remember previous conversations. This is crucial for chatbots!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß† Memory System Created!\n",
      "Memory type: ConversationBufferMemory\n",
      "Current memory: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/4k/bd53pxq144v00900ycf02bch0000gp/T/ipykernel_47143/722529913.py:2: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationBufferMemory()\n"
     ]
    }
   ],
   "source": [
    "# Create a conversation memory\n",
    "memory = ConversationBufferMemory()\n",
    "\n",
    "print(\"üß† Memory System Created!\")\n",
    "print(f\"Memory type: {type(memory).__name__}\")\n",
    "print(f\"Current memory: {memory.buffer}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üí¨ Conversation Chain with Memory Created!\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Create a conversation chain with memory\n",
    "conversation_template = PromptTemplate(\n",
    "    input_variables=[\"history\", \"human_input\"],\n",
    "    template=\"\"\"\n",
    "    You are a helpful AI assistant. Use the conversation history to provide context-aware responses.\n",
    "    \n",
    "    Conversation History:\n",
    "    {history}\n",
    "    \n",
    "    Human: {human_input}\n",
    "    AI: \"\"\"\n",
    ")\n",
    "\n",
    "conversation_chain = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=conversation_template,\n",
    "    memory=memory,\n",
    "    verbose=True  # Shows the chain's thinking process\n",
    ")\n",
    "\n",
    "print(\"üí¨ Conversation Chain with Memory Created!\")\n",
    "print(\"=\"*60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üí¨ Simulated Conversation:\n",
      "============================================================\n",
      "\n",
      "üë§ Human 1: Hi! My name is Alex.\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "    You are a helpful AI assistant. Use the conversation history to provide context-aware responses.\n",
      "    \n",
      "    Conversation History:\n",
      "    \n",
      "    \n",
      "    Human: Hi! My name is Alex.\n",
      "    AI: \u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "ü§ñ AI 1:  Hi Alex! It's nice to meet you. How can I assist you?\n",
      "----------------------------------------\n",
      "\n",
      "üë§ Human 2: I'm learning Python programming.\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "    You are a helpful AI assistant. Use the conversation history to provide context-aware responses.\n",
      "    \n",
      "    Conversation History:\n",
      "    Human: Hi! My name is Alex.\n",
      "AI:  Hi Alex! It's nice to meet you. How can I assist you?\n",
      "    \n",
      "    Human: I'm learning Python programming.\n",
      "    AI: \u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "ü§ñ AI 2: That's great! Is there anything specific you need help with? I can provide resources or answer any questions you may have.\n",
      "----------------------------------------\n",
      "\n",
      "üë§ Human 3: What should I study first?\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "    You are a helpful AI assistant. Use the conversation history to provide context-aware responses.\n",
      "    \n",
      "    Conversation History:\n",
      "    Human: Hi! My name is Alex.\n",
      "AI:  Hi Alex! It's nice to meet you. How can I assist you?\n",
      "Human: I'm learning Python programming.\n",
      "AI: That's great! Is there anything specific you need help with? I can provide resources or answer any questions you may have.\n",
      "    \n",
      "    Human: What should I study first?\n",
      "    AI: \u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "ü§ñ AI 3: It's important to start with the basics of Python, such as data types, variables, and loops. From there, you can move on to more advanced topics like functions and object-oriented programming. I can recommend some online courses or tutorials to help you get started. \n",
      "----------------------------------------\n",
      "\n",
      "üë§ Human 4: Can you remind me what my name is?\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "    You are a helpful AI assistant. Use the conversation history to provide context-aware responses.\n",
      "    \n",
      "    Conversation History:\n",
      "    Human: Hi! My name is Alex.\n",
      "AI:  Hi Alex! It's nice to meet you. How can I assist you?\n",
      "Human: I'm learning Python programming.\n",
      "AI: That's great! Is there anything specific you need help with? I can provide resources or answer any questions you may have.\n",
      "Human: What should I study first?\n",
      "AI: It's important to start with the basics of Python, such as data types, variables, and loops. From there, you can move on to more advanced topics like functions and object-oriented programming. I can recommend some online courses or tutorials to help you get started. \n",
      "    \n",
      "    Human: Can you remind me what my name is?\n",
      "    AI: \u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "ü§ñ AI 4: Of course, your name is Alex. Is there anything else I can assist you with?\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Simulate a conversation\n",
    "conversation_messages = [\n",
    "    \"Hi! My name is Alex.\",\n",
    "    \"I'm learning Python programming.\",\n",
    "    \"What should I study first?\",\n",
    "    \"Can you remind me what my name is?\"\n",
    "]\n",
    "\n",
    "print(\"üí¨ Simulated Conversation:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for i, message in enumerate(conversation_messages, 1):\n",
    "    print(f\"\\nüë§ Human {i}: {message}\")\n",
    "    \n",
    "    try:\n",
    "        response = conversation_chain.run(message)\n",
    "        print(f\"ü§ñ AI {i}: {response}\")\n",
    "    except:\n",
    "        # Simulated responses\n",
    "        if i == 1:\n",
    "            print(\"ü§ñ AI 1: Hello Alex! Nice to meet you! How can I help you today?\")\n",
    "        elif i == 2:\n",
    "            print(\"ü§ñ AI 2: That's great, Alex! Python is an excellent choice for beginners.\")\n",
    "        elif i == 3:\n",
    "            print(\"ü§ñ AI 3: For Python beginners, I'd recommend starting with variables, data types, and basic syntax.\")\n",
    "        elif i == 4:\n",
    "            print(\"ü§ñ AI 4: Your name is Alex! You mentioned it at the beginning of our conversation.\")\n",
    "    \n",
    "    print(\"-\"*40)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß† Current Memory Contents:\n",
      "========================================\n",
      "Human: Hi! My name is Alex.\n",
      "AI:  Hi Alex! It's nice to meet you. How can I assist you?\n",
      "Human: I'm learning Python programming.\n",
      "AI: That's great! Is there anything specific you need help with? I can provide resources or answer any questions you may have.\n",
      "Human: What should I study first?\n",
      "AI: It's important to start with the basics of Python, such as data types, variables, and loops. From there, you can move on to more advanced topics like functions and object-oriented programming. I can recommend some online courses or tutorials to help you get started. \n",
      "Human: Can you remind me what my name is?\n",
      "AI: Of course, your name is Alex. Is there anything else I can assist you with?\n",
      "\n",
      "üìä Memory Statistics:\n",
      "- Total messages: 8\n",
      "- Memory type: ConversationBufferMemory\n"
     ]
    }
   ],
   "source": [
    "# Check what's stored in memory\n",
    "print(\"üß† Current Memory Contents:\")\n",
    "print(\"=\"*40)\n",
    "print(memory.buffer)\n",
    "\n",
    "print(\"\\nüìä Memory Statistics:\")\n",
    "print(f\"- Total messages: {len(memory.chat_memory.messages)}\")\n",
    "print(f\"- Memory type: {type(memory).__name__}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "### üß† Types of Memory in LangChain\n",
    "\n",
    "1. **ConversationBufferMemory**: Stores all messages\n",
    "2. **ConversationSummaryMemory**: Summarizes long conversations\n",
    "3. **ConversationTokenBufferMemory**: Limits memory by token count\n",
    "4. **ConversationBufferWindowMemory**: Keeps only recent messages\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Quick Exercise\n",
    "\n",
    "**Memory Challenge**: If you were building a customer service chatbot, which type of memory would you choose and why?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "**Your Answer:**\n",
    "\n",
    "**Memory Type:** [Your choice]\n",
    "**Reason:** [Why you chose this type]\n",
    "\n",
    "**Hint:** Consider factors like conversation length, privacy, and performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## üéØ Section 5: Real-World Applications\n",
    "\n",
    "Let's build some practical examples that you might actually use!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Code Review Assistant Created!\n",
      "==================================================\n",
      "\n",
      "üìù Code Review:\n",
      "\n",
      "    1. The code takes in a list of numbers and calculates the average of those numbers.\n",
      "    2. - The function could be renamed to better reflect what it does, such as \"calculate_list_average\".\n",
      "       - The function could include error handling for cases where the input is not a list or the list is empty.\n",
      "       - The function could include type checking to ensure that all elements in the list are numbers.\n",
      "    3. - Add comments to explain the purpose of the function and the logic behind it.\n",
      "       - Use descriptive variable names instead of single letters for better readability.\n",
      "       - Consider using the built-in function \"sum()\" instead of a for loop to calculate the total.\n",
      "       - Consider using the built-in function \"statistics.mean()\" instead of manually calculating the average.\n",
      "       - Consider adding a docstring to the function to provide information on its purpose, parameters, and return value.\n"
     ]
    }
   ],
   "source": [
    "# Application 1: Code Review Assistant\n",
    "code_review_template = PromptTemplate(\n",
    "    input_variables=[\"code\", \"language\"],\n",
    "    template=\"\"\"\n",
    "    Review this {language} code and provide feedback:\n",
    "    \n",
    "    Code:\n",
    "    {code}\n",
    "    \n",
    "    Please provide:\n",
    "    1. What the code does\n",
    "    2. Potential improvements\n",
    "    3. Best practices suggestions\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "code_review_chain = LLMChain(llm=llm, prompt=code_review_template)\n",
    "\n",
    "print(\"üîç Code Review Assistant Created!\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Sample code to review\n",
    "sample_code = \"\"\"\n",
    "def calculate_average(numbers):\n",
    "    total = 0\n",
    "    for num in numbers:\n",
    "        total = total + num\n",
    "    return total / len(numbers)\n",
    "\"\"\"\n",
    "\n",
    "try:\n",
    "    review = code_review_chain.run({\"code\": sample_code, \"language\": \"Python\"})\n",
    "    print(\"\\nüìù Code Review:\")\n",
    "    print(review)\n",
    "except:\n",
    "    print(\"\\nüìù Simulated Code Review:\")\n",
    "    print(\"\"\"\n",
    "    1. What the code does: Calculates the average of a list of numbers\n",
    "    \n",
    "    2. Potential improvements:\n",
    "       - Add input validation for empty lists\n",
    "       - Use sum() function instead of manual loop\n",
    "       - Add type hints\n",
    "    \n",
    "    3. Best practices suggestions:\n",
    "       - Handle division by zero\n",
    "       - Add docstring\n",
    "       - Consider using statistics.mean()\n",
    "    \"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìö Study Guide Generator Created!\n",
      "==================================================\n",
      "\n",
      "üìñ Study Guide:\n",
      "\n",
      "1. Key Concepts and Definitions\n",
      "- Function: A set of instructions or statements that perform a specific task.\n",
      "- Parameters: Values that are passed into a function to be used as variables within the function.\n",
      "- Arguments: The actual values that are passed into a function when it is called.\n",
      "- Return: The value that a function outputs after it has completed its task.\n",
      "- Scope: The area of a program where a variable can be accessed.\n",
      "- Local vs Global Variables: Local variables are only accessible within the function they are defined in, while global variables can be accessed from anywhere in the program.\n",
      "- Function Call: The act of using a function in your code by using its name and passing in any necessary arguments.\n",
      "- Recursion: A function calling itself within its own code.\n",
      "\n",
      "2. Important Examples\n",
      "- Basic function syntax:\n",
      "```\n",
      "def function_name(parameters):\n",
      "    # Code block\n",
      "    return value\n",
      "```\n",
      "\n",
      "- Function with no parameters:\n",
      "```\n",
      "def hello():\n",
      "    print(\"Hello!\")\n",
      "```\n",
      "\n",
      "- Function with parameters:\n",
      "```\n",
      "def multiply(x, y):\n",
      "    return x * y\n",
      "```\n",
      "\n",
      "- Function with default parameter values:\n",
      "```\n",
      "def power(base, exponent=2):\n",
      "    return base ** exponent\n",
      "```\n",
      "\n",
      "- Function with keyword arguments:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Application 2: Study Guide Generator\n",
    "study_guide_template = PromptTemplate(\n",
    "    input_variables=[\"topic\", \"level\"],\n",
    "    template=\"\"\"\n",
    "    Create a comprehensive study guide for {topic} at {level} level.\n",
    "    \n",
    "    Include:\n",
    "    1. Key concepts and definitions\n",
    "    2. Important examples\n",
    "    3. Common mistakes to avoid\n",
    "    4. Practice questions\n",
    "    5. Additional resources\n",
    "    \n",
    "    Format it clearly with headers and bullet points.\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "study_guide_chain = LLMChain(llm=llm, prompt=study_guide_template)\n",
    "\n",
    "print(\"üìö Study Guide Generator Created!\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "try:\n",
    "    guide = study_guide_chain.run({\"topic\": \"functions in programming\", \"level\": \"beginner\"})\n",
    "    print(\"\\nüìñ Study Guide:\")\n",
    "    print(guide)\n",
    "except:\n",
    "    print(\"\\nüìñ Simulated Study Guide:\")\n",
    "    print(\"\"\"\n",
    "    # Functions in Programming - Beginner Guide\n",
    "    \n",
    "    ## Key Concepts\n",
    "    - Functions are reusable blocks of code\n",
    "    - They take inputs (parameters) and return outputs\n",
    "    - They help organize and modularize code\n",
    "    \n",
    "    ## Important Examples\n",
    "    ```python\n",
    "    def greet(name):\n",
    "        return f\"Hello, {name}!\"\n",
    "    ```\n",
    "    \n",
    "    ## Common Mistakes\n",
    "    - Forgetting to call the function\n",
    "    - Not returning values when needed\n",
    "    - Incorrect parameter order\n",
    "    \n",
    "    ## Practice Questions\n",
    "    1. What is the difference between parameters and arguments?\n",
    "    2. How do you create a function that returns multiple values?\n",
    "    \n",
    "    ## Additional Resources\n",
    "    - Python documentation on functions\n",
    "    - Online tutorials and practice exercises\n",
    "    \"\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## üéØ Section 6: Advanced Concepts (Preview)\n",
    "\n",
    "Here's a glimpse of what you can do with more advanced LangChain features:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîó Sequential Chain Example:\n",
      "========================================\n",
      "üìã Chain Flow:\n",
      "Interest ‚Üí Topic ‚Üí Questions ‚Üí Answers\n",
      "\n",
      "üéØ This creates a complete learning module automatically!\n"
     ]
    }
   ],
   "source": [
    "# Sequential Chains - Multiple operations in sequence\n",
    "print(\"üîó Sequential Chain Example:\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "# Step 1: Generate a topic\n",
    "topic_template = PromptTemplate(\n",
    "    input_variables=[\"interest\"],\n",
    "    template=\"Generate an interesting topic related to {interest}.\"\n",
    ")\n",
    "\n",
    "# Step 2: Create questions about that topic\n",
    "question_template = PromptTemplate(\n",
    "    input_variables=[\"topic\"],\n",
    "    template=\"Create 3 questions about {topic}.\"\n",
    ")\n",
    "\n",
    "# Step 3: Answer those questions\n",
    "answer_template = PromptTemplate(\n",
    "    input_variables=[\"topic\", \"questions\"],\n",
    "    template=\"\"\"\n",
    "    Topic: {topic}\n",
    "    Questions: {questions}\n",
    "    \n",
    "    Provide detailed answers to each question.\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "print(\"üìã Chain Flow:\")\n",
    "print(\"Interest ‚Üí Topic ‚Üí Questions ‚Üí Answers\")\n",
    "print(\"\\nüéØ This creates a complete learning module automatically!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## üéØ Final Challenge\n",
    "\n",
    "**Design Your Own LangChain Application!**\n",
    "\n",
    "Think about a problem you'd like to solve with AI. Design a LangChain application for it:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "**Your Application Design:**\n",
    "\n",
    "**Application Name:** [Your idea]\n",
    "\n",
    "**Problem it solves:** [What problem does it address?]\n",
    "\n",
    "**Components needed:**\n",
    "- LLM: [Which model?]\n",
    "- Prompts: [What prompts would you use?]\n",
    "- Chains: [How would you structure the flow?]\n",
    "- Memory: [Would it need memory?]\n",
    "- Tools: [Any external integrations?]\n",
    "\n",
    "**Example:**\n",
    "- **Name:** Personal Fitness Coach\n",
    "- **Problem:** Helps users create workout plans\n",
    "- **Components:** LLM for advice, prompts for workout generation, memory for tracking progress, tools for exercise database\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## üéâ Congratulations!\n",
    "\n",
    "You've completed your introduction to LangChain! Here's what you've learned:\n",
    "\n",
    "### ‚úÖ Key Takeaways\n",
    "\n",
    "1. **LangChain Basics**: Understanding the framework and its components\n",
    "2. **LLMs and Prompts**: How to work with language models effectively\n",
    "3. **Chains**: Building reusable AI workflows\n",
    "4. **Memory**: Maintaining context in conversations\n",
    "5. **Real Applications**: Practical examples you can build upon\n",
    "\n",
    "### üöÄ Next Steps\n",
    "\n",
    "1. **Practice**: Try building your own chains and applications\n",
    "2. **Explore**: Check out LangChain's documentation and examples\n",
    "3. **Experiment**: Combine different components to create unique solutions\n",
    "4. **Learn More**: Dive into advanced topics like agents, tools, and embeddings\n",
    "\n",
    "### üìö Resources\n",
    "\n",
    "- [LangChain Documentation](https://python.langchain.com/)\n",
    "- [LangChain GitHub](https://github.com/langchain-ai/langchain)\n",
    "- [OpenAI API Documentation](https://platform.openai.com/docs)\n",
    "- [Prompt Engineering Guide](https://www.promptingguide.ai/)\n",
    "\n",
    "### üéØ Remember\n",
    "\n",
    "LangChain is like a toolkit - the more you practice, the more powerful your AI applications become! Start small, experiment often, and don't be afraid to make mistakes. Every great AI application started with a simple chain. üöÄ\n",
    "\n",
    "**Happy coding!** üíª‚ú®\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## üìù Additional Exercises\n",
    "\n",
    "Try these exercises to reinforce your learning:\n",
    "\n",
    "### Exercise 1: Email Generator\n",
    "Create a chain that generates professional emails based on the recipient and purpose.\n",
    "\n",
    "### Exercise 2: Language Translator\n",
    "Build a chain that translates text between different languages.\n",
    "\n",
    "### Exercise 3: Quiz Generator\n",
    "Create a system that generates quizzes on any topic with multiple-choice questions.\n",
    "\n",
    "### Exercise 4: Code Debugger\n",
    "Build a chain that helps debug Python code by analyzing error messages.\n",
    "\n",
    "### Exercise 5: Personal Assistant\n",
    "Design a conversational assistant that can help with daily tasks and remember user preferences.\n",
    "\n",
    "---\n",
    "\n",
    "**Share your creations and discoveries with the community!** üåü\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
